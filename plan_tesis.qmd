---
title: "UTN Regional Paran√° - Maestr√≠a en Miner√≠a de Datos"
subtitle: "Plan de Tesis"
date: "Octubre 20223"
format:
  pdf:
    code-fold: false    
jupyter: python3
bibliography: references.bib
---

## Directores: Dr. Mat√≠as Gerard y Dr. Leandro Vignolo (CONICET-UNL)

### T√≠tulo: "Aplicaci√≥n de autoencoders variacionales para mejorar los procesos de optimizaci√≥n evolutiva multiobjetivo"

### **Alumno: Lic. Claudio Sebasti√°n Castillo (UTN)**

\pagebreak

## Fundamentaci√≥n y Justificaci√≥n del tema

Los algoritmos evolutivos (en adelante AE) son m√©todos heur√≠sticos de optimizaci√≥n inspirados en la evoluci√≥n natural, dise√±ados para encontrar soluciones en espacios de b√∫squeda complejos . A diferencia de los m√©todos de optimizaci√≥n exhaustivos (e.g. *full search*), los AE son particularmente efectivos en espacios de b√∫squeda discretos, ruidosos o cuando la funci√≥n objetivo es desconocida o no diferenciable [@goldbergdavide.GeneticAlgorithmsSearch1989]. Utilizando t√©cnicas *evolutivas* estos algoritmos generan iterativamente soluciones a partir de una poblaci√≥n de candidatos, de manera similar a c√≥mo la evoluci√≥n natural optimiza caracter√≠sticas biol√≥gicas a lo largo de nuevas generaciones. Aplicados a problemas multiobjetivo sus resultados regularmente implican soluciones cercanas al √≥ptimo, que mantienen un buen compromiso en la satisfacci√≥n de m√∫ltiples requerimientos [@jiaoSurveyEvolutionaryMultiobjective2023].

La robustez de los AE est√° determinada, como bien sostiene Goldberg [-@goldbergdavide.GeneticAlgorithmsSearch1989], por una serie de caracter√≠sticas distintivas, que fortalecen su configuraci√≥n de b√∫squeda, a saber: a) operan sobre un espacio *codificado* del problema y no sobre el espacio en su representaci√≥n original; b) avanzan evaluando una *poblaci√≥n de soluciones* y no soluciones individuales; c) tienen como gu√≠a una *funci√≥n de aptitud* que no incluye derivaci√≥n u otras funciones de c√°lculo; y d) suponen *reglas probabil√≠sticas de transici√≥n* (operadores estoc√°sticos) y no t√©cnicas determin√≠sticas. Estas caracter√≠sticas permiten a los AE superar restricciones que tienen otros m√©todos de optimizaci√≥n, condicionados -por ejemplo- a espacios de b√∫squeda continuos, diferenciables o unimodales. Por ello, su aplicaci√≥n se ha extendido notablemente fuera del campo de la optimizaci√≥n, aplic√°ndose en distintas tareas [@vieQualitiesChallengesFuture2021] y a lo largo de diversas industrias [@jiaoSurveyEvolutionaryMultiobjective2023].

A pesar de su robustez los AE se enfrentan a problemas desafiantes cuando se aplican a *datasets* de alta dimensionalidad y bajo n√∫mero de muestras. En efecto, como sucede con gran parte de los algoritmos de aprendizaje autom√°tico (en adelante AA), en espacios de alta dimensionalidad la cardinalidad del conjunto de soluciones candidatas crece de manera exponencial, y los problemas se vuelven computacionalmente intratables. Cabe destacar que para un conjunto de `n` caracter√≠sticas es posible determinar un total de `n2` posibles soluciones, espacio que constituye un dominio de b√∫squeda dif√≠cil de cubrir a√∫n con `n` conservadores.[^1] Al mismo tiempo, la escasez de datos afecta gravemente el proceso de optimizaci√≥n, limitando la capacidad informativa de la funci√≥n objetivo [@hastieElementStatisticalLearning2009]. En este sentido, la reconocida *maldici√≥n de la dimensionalidad*[^2] tambi√©n acecha a los AE pues a medida que las soluciones ganan complejidad (atributos) se requieren mayor cantidad de muestras (datos) para que la funci√≥n objetivo aporte direcciones valiosas en el espacio de b√∫squeda y escape a soluciones sub√≥ptimas [@hamadaDataDrivenAnalysisPareto2018].

[^1]: Por ejemplo para un dataset de 20 caracter√≠sticas (atributos) el n√∫mero total de subconjuntos a evaluar supera el mill√≥n de posibles candidatos, espec√≠ficamente: 1.048.576.

[^2]: Termino acu√±ado por Richard Bellman (1961) y que en el mundo del *aprendizaje autom√°tico* refiere a los problemas y desaf√≠os que presentan los datos organizados en espacios de altas dimensiones (m√∫ltiples atributos). En tales espacios, la cantidad de datos necesarios para proporcionar una representaci√≥n estad√≠sticamente confiable crece exponencialmente pues toda representaci√≥n tendr√° gran dispersi√≥n entre sus datos. Esto significa que para cualquier conjunto de individuos la distancia promedio entre ellos aumenta a medida que aumentan las dimensiones, haciendo que cada uno se separe de los otros cada vez m√°s.

Por lo dicho, la disponibilidad de datos muestrales es un aspecto cr√≠tico de los algoritmos de AA en general y de los AE en particular, jugando un papel central en muchas situaciones pr√°cticas. As√≠ pues, resulta clave en contextos de optimizaci√≥n en el que se emplean arquitecturas neuronales profundas [@alzubaidiSurveyDeepLearning2023], pero tambi√©n en otros escenarios donde, sin llegar a modelos muy complejos, se dispone de un conjunto de datos cuyas clases se encuentran desbalanceadas [@wongUnderstandingDataAugmentation2016, @blagusSMOTEHighdimensionalClassimbalanced2013]. La distribuci√≥n inequitativa de datos entre clases es frecuente en sectores donde los eventos excepcionales son aquellos que despiertan mayor inter√©s (e.g. sectores como salud, seguridad, telecomunicaciones y finanzas, son algunos ejemplos[@fajardoOversamplingImbalancedData2021]). Mientras que las redes neuronales profundas y m√°s ampliamente el tama√±o de los modelos neuronales no han parado de crecer en aptitud y complejidad.[^3]

[^3]: Baste citar como ejemplo muy popular en estos d√≠as al modelo *Transformer* y su enorme impacto dentro del mundo del AA y el NLP.

Todo esto ha impulsado la b√∫squeda de estrategias de **aumentaci√≥n** de datos que permitan superar los problemas derivados de su escasez y trabajar de manera intensiva con la informaci√≥n disponible [@gmComprehensiveSurveyAnalysis2020]. Entre esas estrategias el uso de Autoencoders Variacionales (en adelante AV) ocupa un lugar prominente. En efecto, los AV constituyen modelos generativos (redes neuronales profundas con arquitectura *encoder-decoder*) capaces de aprender una representaci√≥n latente de datos disponibles y generar nuevas muestras con las mismas caracter√≠sticas fundamentales (i.e. similar distribuci√≥n conjunta de probabilidad) que los datos originales [@kingmaIntroductionVariationalAutoencoders2019]. Esta capacidad generativa, fundada en la aptitud para modelar la distribuci√≥n de probabilidad de ciertos datos, es particularmente efectiva por el hecho de que prescinde de fuertes supuestos estad√≠sticos a los que adscriben otros modelos y por su *escalabilidad* gracias a emplear *retropropagaci√≥n* como estrategia de optimizaci√≥n [@doerschTutorialVariationalAutoencoders2021, @kingmaIntroductionVariationalAutoencoders2019]. Hoy los AV son ampliamente utilizados en biolog√≠a molecular, qu√≠mica, procesamiento de lenguaje natural, astronom√≠a, entre otros [@ramchandranLearningConditionalVariational2022].

Por lo dicho hasta aqu√≠, la posibilidad de expandir el conjunto de datos mediante el uso de AV, abre alternativas para mejorar el proceso de optimizaci√≥n de los AE. Esta contribuci√≥n no solo estar√≠a en permitir una mayor exploraci√≥n del espacio de soluciones aplicando operadores evolutivos, sino tambi√©n en mitigar riesgos de sobreajuste y convergencia prematura a soluciones sub√≥ptimas. De este modo, la integraci√≥n de ambas tecnolog√≠as ofrece un enfoque prometedor para abordar escenarios de alta dimensionalidad y muestras escasas. A la fecha de publicaci√≥n de la presente no hemos encontrado experiencias publicadas de aplicaci√≥n general de AV en el √°mbito de AE. En la medida que esto sea as√≠ creemos que nuestro aporte a la comunidad de investigadores y practicantes de la disciplina estar√° en proveer informaci√≥n y experimentaci√≥n sobre la combinaci√≥n de estos algoritmos largamente utilizados.

pendiente: introducir GA en EA [@vignoloEvolutionaryLocalImprovement2017]

## Estado del arte

AV en general

-   [@fajardoOversamplingImbalancedData2021]

-   [@kwarciakDeepGenerativeNetworks2023]

-   [@leelarathnaEnhancingRepresentationLearning2023]

-   [@aiGenerativeOversamplingImbalanced2023]

-   [@tranArtificialNeuralNetworkbased2022]

-   [@khmaissiaConfidenceGuidedDataAugmentation2023]

AV en el contexto del AE

-   [@martinsVariationalAutoencodersEvolutionary2022]

-   [@wuEVAEEvolutionaryVariational2023]

GA y b√∫squeda local

-   [@vignoloEvolutionaryLocalImprovement2017]

## Definici√≥n del problema

La escasez de datos muestrales en procesos de optimizaci√≥n evolutiva multiobjetivo y su posible soluci√≥n a partir de t√©cnicas de aumentaci√≥n de datos mediante autoencoders variacionales.

## Objetivos

**General**:

1\. Investigar la eficacia de los autoencoders variacionales en la aumentaci√≥n de datos para mejorar los procesos de optimizaci√≥n evolutiva multiobjetivo.

**Espec√≠ficos**:\
1. Implementar una arquitectura que integre autoencoders variacionales en la etapa inicial de un algoritmo evolutivo para la generaci√≥n aumentada de poblaci√≥n en vistas a la optimizaci√≥n.\
2. Implementar una arquitectura que integre autoencoders variacionales en el contexto de optimizaci√≥n evolutiva, particularmente en la etapa de evoluci√≥n de poblaciones, para favorecer el proceso de b√∫squeda de subconjuntos de caracter√≠sticas potencialmente valiosas.

## Metodolog√≠as

Para cumplir los objetivos propuestos compararemos dos algoritmos gen√©ticos evolutivos evaluando sus desempe√±os y resultados. Por un lado implementaremos un algoritmo gen√©tico *cl√°sico* aplicado a conjuntos de datos conocidos dentro de la comunidad cient√≠fica y en condiciones regulares de procesamiento para la resoluci√≥n de problemas de clasificaci√≥n. Por el otro, aplicaremos a los mismos conjuntos un variante *novedosa* de algoritmo gen√©tico, integrando un m√≥dulo de aumentaci√≥n de datos a partir de la intervenci√≥n de *autoencoders variacionales*. Para evaluar esta variante de algoritmo gen√©tico realizaremos diversos experimentos con ajustes en el dise√±o de la arquitectura que consistir√°n en la integraci√≥n del m√≥dulo de aumentaci√≥n de datos en dos puntos: 1) previo al inicio del proceso evolutivo y 2) durante el proceso evolutivo del algoritmo gen√©tico. Para la comparaci√≥n de ambas implementaciones elegiremos como m√©tricas la precisi√≥n (*accuracy*) en la clasificaci√≥n y el tama√±o final de caracter√≠sticas seleccionadas. A continuaci√≥n detallamos los aspectos t√©cnicos vinculados a los elementos mencionados, a saber: algoritmos, datos, arquitecturas y m√©tricas para la evaluaci√≥n.

1.  **Algoritmos:** Para el presente trabajo usaremos algoritmos gen√©ticos (AG) como m√©todo evolutivo[^4] debido a la posibilidad que brinda este algoritmo de emplear codificaci√≥n binaria y permitir as√≠ una representaci√≥n intuitiva del espacio de caracter√≠sticas [@vignoloEvolutionaryLocalImprovement2017]. Para aumentaci√≥n de datos utilizaremos *autoencoders variacionales* (AV) como instancia generativa [@kingmaIntroductionVariationalAutoencoders2019].
    1.  Los AGs constituyen una de las herramientas m√°s estudiadas e implementadas dentro de los m√©todos evolutivos [@goldbergdavide.GeneticAlgorithmsSearch1989, @kramerGeneticAlgorithmEssentials2017], dada su capacidad para encontrar soluciones en espacios de b√∫squeda complejos [@vignoloEvolutionaryLocalImprovement2017]. El procedimiento de b√∫squeda de los AGs opera evolucionando una poblaci√≥n de individuos que consisten en cromosomas que codifican el espacio de soluciones. Dicha evoluci√≥n -al igual que la evoluci√≥n natural- sucede a trav√©s de operadores (funciones) de selecci√≥n, variaci√≥n (mutaci√≥n y cruce) y reemplazo que transforman el material gen√©tico disponible: los individuos m√°s aptos sobreviven y se reproducen, mientras que los menos aptos desaparecen[^5]. Esta aptitud -que imita la presi√≥n selectiva de un entorno natural- se eval√∫a mediante la aplicaci√≥n de una funci√≥n objetivo (espec√≠fica del problema) a cada individuo a partir de la informaci√≥n decodificada de sus cromosomas. Este m√©todo heur√≠stico de b√∫squeda tendr√° en nuestro trabajo dos configuraciones: una *cl√°sica* sin aumentaci√≥n de datos y una *novedosa* con aumentaci√≥n de datos aplicando *autoencoders variacionales* (AV)*.*

    2.  Los AVs son modelos generativos (redes neuronales profundas con arquitectura *encoder-decoder*) capaces de aprender una representaci√≥n latente de datos disponibles y generar nuevas muestras de similares caracter√≠sticas a los datos originales. Estos modelos se basan en el supuesto de que cualquier dato disponible, por ejemplo ùë•, se genera mediante un proceso aleatorio que involucra una variable latente ùëß. Bajo ese supuesto, el modelo procede tomando como muestra una observaci√≥n de ùëß de la distribuci√≥n previa de probabilidad ùëùùúÉ(ùëß), que luego se utiliza para tomar una observaci√≥n de ùë• de la distribuci√≥n condicional ùëùùúÉ(ùë•\|ùëß). El objetivo del modelo es obtener *estimaciones de m√°xima verosimilitud* (tambi√©n llamadas *estimaciones de m√°xima probabilidad a posteriori)* de los par√°metros ùúÉ en situaciones donde tanto la verosimilitud marginal ùëùùúÉ(ùë•) = ‚à´ ùëùùúÉ(ùëß)ùëùùúÉ(ùë•\|ùëß) dùëß como la probabilidad *a posteriori* ùëùùúÉ(ùëß\|ùë•) son intratables [@kingmaIntroductionVariationalAutoencoders2019]. Para eso, utiliza la distribuci√≥n ùëûùúô(ùëß\|ùë•) como una aproximaci√≥n al intratable ùëùùúÉ(ùëß\|ùë•), maximizando el *l√≠mite inferior variacional* para ùëùùúÉ(ùë•) .
2.  **Conjuntos de Datos**. Se llevar√°n a cabo experimentos utilizando cuatro conjuntos de datos con alta dimensionalidad y diferentes caracter√≠sticas, incluyendo un diverso n√∫mero de caracter√≠sticas/atributos, ejemplos y clases/categor√≠as.
    1.  *Madelon*: Este es un conjunto de datos artificial con 500 caracter√≠sticas, donde el objetivo es un XOR multidimensional con cinco caracter√≠sticas relevantes. Fue creado para el desaf√≠o de Selecci√≥n de Caracter√≠sticas NIPS 2003[^6], y est√° disponible en el Repositorio UCI[^7]. De las 495 caracter√≠sticas restantes, 15 corresponden a combinaciones lineales de las cinco relevantes, y las otras 480 son caracter√≠sticas de ruido. Madelon es un problema de clasificaci√≥n de dos clases con variables de entrada binarias dispersas. Las dos clases est√°n equilibradas, y los datos se dividen en conjuntos de entrenamiento y prueba.
    2.  *Leukemia*: El an√°lisis de datos de expresi√≥n g√©nica obtenidos de micro-datos de ADN se estudia en Golub [-@golubMolecularClassificationCancer1999] para la clasificaci√≥n de tipos de c√°ncer. Construyeron un conjunto de datos con 7129 mediciones de expresi√≥n g√©nica en las clases ALL (leucemia linfoc√≠tica aguda) y AML (leucemia mielog√©nica aguda). El problema es distinguir entre estas dos variantes de leucemia (ALL y AML). Los datos se dividen originalmente en dos subconjuntos: un conjunto de entrenamiento y un conjunto de prueba independiente. El conjunto de entrenamiento consta de 38 muestras (27 ALL y 11 AML) de espec√≠menes de m√©dula √≥sea. El conjunto de prueba tiene 34 muestras (20 ALL y 14 AML), preparadas bajo diferentes condiciones experimentales e incluyendo 24 espec√≠menes de m√©dula √≥sea y 10 muestras de sangre.
    3.  *GCM*: El conjunto de datos GCM fue compilado en Ramaswamy [-@ramaswamyMulticlassCancerDiagnosis2001] y contiene los perfiles de expresi√≥n de 198 muestras de tumores que representan 14 clases comunes de c√°ncer humano3. Aqu√≠ el enfoque estuvo en 190 muestras de tumores despu√©s de excluir 8 muestras de met√°stasis, y el preprocesamiento se realiz√≥ de acuerdo con \[24\]. Finalmente, cada matriz se estandariz√≥ a una media de 0 y una varianza de 1 seg√∫n \[25\]. El conjunto de datos consta de un total de 190 instancias, con 16063 atributos (biomarcadores) cada una, y distribuidos en 14 clases desequilibradas. En nuestros experimentos, los datos se separaron en tres conjuntos: entrenamiento (72 muestras), validaci√≥n (72 muestras) y prueba (42 muestras). Con el objetivo de equilibrar las clases en el conjunto de entrenamiento y prevenir el sobreajuste del clasificador debido a la clase mayoritaria, las muestras se repitieron para obtener el mismo n√∫mero para cada clase. Al final, se utilizaron un total de 168 muestras para el entrenamiento. Por el contrario, los conjuntos de validaci√≥n y prueba est√°n estratificados, manteniendo el desequilibrio de clases.
    4.  *Gisette:* es un dataset creado para trabajar el problema de reconocimiento de d√≠gitos escritos a mano [@isabelleguyonGisette2004]. Este conjunto de datos forma parte de los cinco conjuntos utilizados en el desaf√≠o de selecci√≥n de caracter√≠sticas de NIPS 2003. Tiene 13500 observaciones y 5000 atributos. El desaf√≠o radica en diferenciar los d√≠gitos '4' y '9', que suelen ser f√°cilmente confundibles entre s√≠. Los d√≠gitos han sido normalizados en tama√±o y centrados en una imagen fija de 28x28 p√≠xeles. Para el desaf√≠o de selecci√≥n de caracter√≠sticas, se modificaron los datos originales. Espec√≠ficamente, se muestrearon p√≠xeles al azar en la parte superior central de la caracter√≠stica que contiene la informaci√≥n necesaria para diferenciar el 4 del 9. Adem√°s, se crearon caracter√≠sticas de orden superior como productos de estos p√≠xeles para sumergir el problema en un espacio de caracter√≠sticas de mayor dimensi√≥n. Tambi√©n se a√±adieron caracter√≠sticas distractoras denominadas "sondas", que no tienen poder predictivo. El orden de las caracter√≠sticas y patrones fue aleatorizado.
3.  **Arquitecturas**: Para nuestro trabajo utilizaremos dos arquitecturas: una *cl√°sica* sin aumentaci√≥n de datos y una *novedosa* con aumentaci√≥n de datos aplicando *autoencoders variacionales* (AV). A su vez, dentro de esta segunda configuraci√≥n ensayaremos ajustes en el dise√±o que consistir√°n en la integraci√≥n del m√≥dulo de aumentaci√≥n de datos en dos puntos: 1) previo al inicio del proceso evolutivo y 2) durante el proceso evolutivo del algoritmo gen√©tico. De esta forma buscamos evaluar el aporte generativo del modelo AV en distintas etapas del AG.
4.  **M√©tricas de evaluaci√≥n**: Para la comparaci√≥n de ambas implementaciones elegiremos como m√©tricas la precisi√≥n en la clasificaci√≥n (*accuracy*) y el tama√±o final de caracter√≠sticas seleccionadas por el modelo gen√©tico. Generalmente los AGs constituye modelos *multiobjetivo* donde no solo interesa optimizar una m√©trica particular vinculada a su eficacia -por ejemplo precisi√≥n en su clasificaci√≥n de individuos- sino tambi√©n interesa satisfacer requerimientos de eficiencia -como por ejemplo minimizar el conjunto de caracter√≠sticas con que opera el modelo- [@jiaoSurveyEvolutionaryMultiobjective2023]. Por esta raz√≥n nuestra evaluaci√≥n tomar√° en cuenta ambas dimensiones en el desempe√±o de las soluciones propuestas.

[^4]: Otros m√©todos evolutivos robustos, como por ejemplo el *enjambre de part√≠culas* (PSO) y *optimizaci√≥n de colonia de hormigas* (ACO), t√≠picamente utilizan codificaci√≥n basada en n√∫meros reales por lo que constituyen opciones menos adecuadas al problema que enfrentaremos en este trabajo.

[^5]: Como su nombre lo indica el operador de selecci√≥n determina la elegibilidad de un individuo para sobrevivir y reproducirse en funci√≥n de su aptitud para resolver un problema. En el contexto de los AGs esta aptitud no es otra cosa que el puntaje que obtiene un individuo evaluado en una funci√≥n objetivo. Por su parte los operadores de variaci√≥n tienen como funci√≥n combinar la informaci√≥n gen√©tica de individuos (cruce) y alterar aleatoriamente sus cromosomas (mutaci√≥n), promoviendo transformaciones en el material gen√©tico global con sesgo hacia mejorar la aptitud poblacional para resolver un problema. La variaci√≥n equivale a la b√∫squeda natural por mejorar las adaptaciones de los individuos a su entorno. Finalmente el operador de reemplazo mantiene la poblaci√≥n constante, sustituyendo individuos poco aptos por aquellos de mayor aptitud. Estos operadores se combinan en ciclos iterativos que se repiten hasta satisfacer un criterio de terminaci√≥n deseado (por ejemplo, un n√∫mero predefinido de generaciones o un valor de aptitud) [@vignoloEvolutionaryLocalImprovement2017].

[^6]: <http://clopinet.com/isabelle/Projects/NIPS2003/>

[^7]: <http://archive.ics.uci.edu/ml/datasets.html>

## Cronograma de trabajo

\[pendiente\]

## Condiciones para el desarrollo

\[pendiente\]

```{=html}
<!-- 

¬øQu√© presentar?
‚Ä¢ Plan de tesis
‚Ä¢ Compromiso y CV del director
‚Ä¢ CV del Tesista
Recibe y hace una primera evaluaci√≥n el Comit√© Acad√©mico. De tener el visto bueno se eleva para su evaluaci√≥n en CS. Secciones del plan de Maestr√≠a seg√∫n normativa:
Escribir a futuro. 

Extensi√≥n m√°xima recomendada: 8 p√°ginas.

T√≠tulo
‚Ä¢ Debe expresar claramente el tema con rigurosidad y precisi√≥n t√©cnica.

Fundamentaci√≥n y justificaci√≥n del tema (extensi√≥n m√°xima 2 p√°ginas)
‚Ä¢ Marco te√≥rico.
‚Ä¢ Valor cient√≠fico del trabajo propuesto.
‚Ä¢ Alcance.

Estado del arte (extensi√≥n m√°xima sugerida 2 p√°ginas)
‚Ä¢ Evoluci√≥n hist√≥rica y actual del conocimiento.
‚Ä¢ Aspectos o conocimiento que se encuentre vacantes. 
- Citar a mis directores!

Objetivos (extensi√≥n m√°xima sugerida 1 p√°ginas) Preferentemente 1‚ÅÑ2 p√°gina
‚Ä¢ Enunciado claro de los objetivos que den fe de los alcances y l√≠mites.
‚Ä¢ OG ‚Üí l√≠nea de investigaci√≥n
‚Ä¢ OE ‚Üí resultados esperados

Metodolog√≠a y Actividades (extensi√≥n m√°xima sugerida 2 p√°ginas) Lo que necesiten
- Seleccion de datos, como se prepararon los datos, estructura de los datos y Evaluaci√≥n
- Implementaci√≥n
‚Ä¢ Actividades de desarrollar
‚Ä¢ Metodolog√≠as t√©cnicas.
Ac√° agregar cuestiones √©ticas en caso que utilicen datos de humanos (c√≥mo los anonimizan, etc.).

Cronograma de trabajo (extensi√≥n m√°xima sugerida 2 p√°ginas) ‚Üí Preferentemente 1‚ÅÑ2 o 1 p√°gina.
‚Ä¢ Listar actividades con un diagrama de Gantt

Referencias bibliogr√°ficas (extensi√≥n m√°xima sugerida 2 p√°ginas)‚Üí Unas 20-25 como m√°ximo.
‚Ä¢ Actualizadas y pertinentes
‚Ä¢ Elegir y respetar estilo (APA o IEEE)

Condiciones para el desarrollo (extensi√≥n m√°xima sugerida 1 p√°ginas) ‚Üí 1‚ÅÑ2 p√°gina
‚Ä¢ Mencionar y justificar la sede en la que se trabajar√°
‚Ä¢ Equipamiento
‚Ä¢ Software

-->
```
\pagebreak

### Bibliograf√≠a