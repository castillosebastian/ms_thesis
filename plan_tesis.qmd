---
title: "UTN Regional Paraná - Maestría en Minería de Datos"
subtitle: "Plan de Tesis"
date: "Octubre 20223"
format:
  pdf:
    code-fold: false
jupyter: python3
bibliography: references.bib
---

## Directores: Dr. Matías Gerard y Dr. Leandro Vignolo (CONICET-UNL)

### Título: "Aplicación de autoencoders variacionales para mejorar los procesos de optimización evolutiva multiobjetivo"

### **Alumno: Lic. Claudio Sebastián Castillo (UTN)**

\pagebreak

## Fundamentación y Justificación del tema

Los algoritmos evolutivos (en adelante AE) son métodos heurísticos de optimización inspirados en la evolución natural, diseñados para encontrar soluciones en espacios de búsqueda complejos . A diferencia de los métodos de optimización exhaustivos (e.g. *full search*), los AE son particularmente efectivos en espacios de búsqueda discretos, ruidosos o cuando la función objetivo es desconocida o no diferenciable [@goldbergdavide.GeneticAlgorithmsSearch1989]. Utilizando técnicas *evolutivas* estos algoritmos generan iterativamente soluciones a partir de una población de candidatos, de manera similar a cómo la evolución natural optimiza características biológicas a lo largo de nuevas generaciones. Aplicados a problemas multiobjetivo sus resultados regularmente implican soluciones cercanas al óptimo, que mantienen un buen compromiso en la satisfacción de múltiples requerimientos [@jiaoSurveyEvolutionaryMultiobjective2023].

La robustez de los AE está determinada, como bien sostiene Goldberg [-@goldbergdavide.GeneticAlgorithmsSearch1989], por una serie de características distintivas, que fortalecen su configuración de búsqueda, a saber: a) operan sobre un espacio *codificado* del problema y no sobre el espacio en su representación original; b) avanzan evaluando una *población de soluciones* y no soluciones individuales; c) tienen como guía una *función de aptitud* que no incluye derivación u otras funciones de cálculo; y d) suponen *reglas probabilísticas de transición* (operadores estocásticos) y no técnicas determinísticas. Estas características permiten a los AE superar restricciones que tienen otros métodos de optimización, condicionados -por ejemplo- a espacios de búsqueda continuos, diferenciables o unimodales. Por ello, su aplicación se ha extendido notablemente fuera del campo de la optimización, aplicándose en distintas tareas [@vieQualitiesChallengesFuture2021] y a lo largo de diversas industrias [@jiaoSurveyEvolutionaryMultiobjective2023].

A pesar de su robustez los AE se enfrentan a problemas desafiantes cuando se aplican a *datasets* de alta dimensionalidad y bajo número de muestras. En efecto, como sucede con gran parte de los algoritmos de aprendizaje automático (en adelante AA), en espacios de alta dimensionalidad la cardinalidad del conjunto de soluciones candidatas crece de manera exponencial, y los problemas se vuelven computacionalmente intratables. Cabe destacar que para un conjunto de `n` características es posible determinar un total de `n2` posibles soluciones, espacio que constituye un dominio de búsqueda difícil de cubrir aún con `n` conservadores.[^1] Al mismo tiempo, la escasez de datos afecta gravemente el proceso de optimización, limitando la capacidad informativa de la función objetivo [@hastieElementStatisticalLearning2009]. En este sentido, la reconocida *maldición de la dimensionalidad*[^2] también acecha a los AE pues a medida que las soluciones ganan complejidad (atributos) se requieren mayor cantidad de muestras (datos) para que la función objetivo aporte direcciones valiosas en el espacio de búsqueda y escape a soluciones subóptimas [@hamadaDataDrivenAnalysisPareto2018].

[^1]: Por ejemplo para un dataset de 20 características (atributos) el número total de subconjuntos a evaluar supera el millón de posibles candidatos, específicamente: 1.048.576.

[^2]: Termino acuñado por Richard Bellman (1961) y que en el mundo del *aprendizaje automático* refiere a los problemas y desafíos que presentan los datos organizados en espacios de altas dimensiones (múltiples atributos). En tales espacios, la cantidad de datos necesarios para proporcionar una representación estadísticamente confiable crece exponencialmente pues toda representación tendrá gran dispersión entre sus datos. Esto significa que para cualquier conjunto de individuos la distancia promedio entre ellos aumenta a medida que aumentan las dimensiones, haciendo que cada uno se separe de los otros cada vez más.

Por lo dicho, la disponibilidad de datos muestrales es un aspecto crítico de los algoritmos de AA en general y de los AE en particular, jugando un papel central en muchas situaciones prácticas. Así pues, resulta clave en contextos de optimización en el que se emplean arquitecturas neuronales profundas [@alzubaidiSurveyDeepLearning2023], pero también en otros escenarios donde, sin llegar a modelos muy complejos, se dispone de un conjunto de datos cuyas clases se encuentran desbalanceadas [@wongUnderstandingDataAugmentation2016, @blagusSMOTEHighdimensionalClassimbalanced2013]. La distribución inequitativa de datos entre clases es frecuente en sectores donde los eventos excepcionales son aquellos que despiertan mayor interés (e.g. sectores como salud, seguridad, telecomunicaciones y finanzas, son algunos ejemplos[@fajardoOversamplingImbalancedData2021]). Mientras que las redes neuronales profundas y más ampliamente el tamaño de los modelos neuronales no han parado de crecer en aptitud y complejidad.[^3]

[^3]: Baste citar como ejemplo muy popular en estos días al modelo *Transformer* y su enorme impacto dentro del mundo del AA y el NLP.

Todo esto ha impulsado la búsqueda de estrategias de **aumentación** de datos que permitan superar los problemas derivados de su escasez y trabajar de manera intensiva con la información disponible [@gmComprehensiveSurveyAnalysis2020]. Entre esas estrategias el uso de Autoencoders Variacionales (en adelante AV) ocupa un lugar prominente. En efecto, los AV constituyen modelos generativos (redes neuronales profundas con arquitectura *encoder-decoder*) capaces de aprender una representación latente de datos disponibles y generar nuevas muestras con las mismas características fundamentales (i.e. similar distribución conjunta de probabilidad) que los datos originales [@kingmaIntroductionVariationalAutoencoders2019]. Esta capacidad generativa, fundada en la aptitud para modelar la distribución de probabilidad de ciertos datos, es particularmente efectiva por el hecho de que prescinde de fuertes supuestos estadísticos a los que adscriben otros modelos y por su *escalabilidad* gracias a emplear *retropropagación* como estrategia de optimización [@doerschTutorialVariationalAutoencoders2021, @kingmaIntroductionVariationalAutoencoders2019]. Hoy los AV son ampliamente utilizados en biología molecular, química, procesamiento de lenguaje natural, astronomía, entre otros [@ramchandranLearningConditionalVariational2022].

Por lo dicho hasta aquí, la posibilidad de expandir el conjunto de datos mediante el uso de AV, abre alternativas para mejorar el proceso de optimización de los AE. Esta contribución no solo estaría en permitir una mayor exploración del espacio de soluciones aplicando operadores evolutivos, sino también en mitigar riesgos de sobreajuste y convergencia prematura a soluciones subóptimas. De este modo, la integración de ambas tecnologías ofrece un enfoque prometedor para abordar escenarios de alta dimensionalidad y muestras escasas. A la fecha de publicación de la presente no hemos encontrado experiencias publicadas de aplicación general de AV en el ámbito de AE. En la medida que esto sea así creemos que nuestro aporte a la comunidad de investigadores y practicantes de la disciplina estará en proveer información y experimentación sobre la combinación de estos algoritmos largamente utilizados.

pendiente: introducir GA en EA [@vignoloEvolutionaryLocalImprovement2017]

## Estado del arte

AV en general

-   [@fajardoOversamplingImbalancedData2021]

-   [@kwarciakDeepGenerativeNetworks2023]

-   [@leelarathnaEnhancingRepresentationLearning2023]

-   [@aiGenerativeOversamplingImbalanced2023]

-   [@tranArtificialNeuralNetworkbased2022]

-   [@khmaissiaConfidenceGuidedDataAugmentation2023]

AV en el contexto del AE

-   [@martinsVariationalAutoencodersEvolutionary2022]

-   [@wuEVAEEvolutionaryVariational2023]

GA y búsqueda local

-   [@vignoloEvolutionaryLocalImprovement2017]

## Definición del problema

La escasez de datos muestrales en procesos de optimización evolutiva multiobjetivo y su posible solución a partir de técnicas de aumentación de datos mediante autoencoders variacionales.

## Objetivos

**General**:

1\. Investigar la eficacia de los autoencoders variacionales en la aumentación de datos para mejorar los procesos de optimización evolutiva multiobjetivo.

**Específicos**:\
1. Implementar una arquitectura que integre autoencoders variacionales en la etapa inicial de un algoritmo evolutivo para la generación aumentada de población en vistas a la optimización.\
2. Implementar una arquitectura que integre autoencoders variacionales en el contexto de optimización evolutiva, particularmente en la etapa de evolución de poblaciones, para favorecer el proceso de búsqueda de subconjuntos de características potencialmente valiosas.

## Metodologías

1.  **Algoritmos**
2.  **Conjuntos de Datos**. Se llevarán a cabo experimentos utilizando cuatro conjuntos de datos con alta dimensionalidad y diferentes características, incluyendo un diverso número de características/atributos, ejemplos y clases/categorías.
    1.  *Madelon*: Este es un conjunto de datos artificial con 500 características, donde el objetivo es un XOR multidimensional con cinco características relevantes. Fue creado para el desafío de Selección de Características NIPS 2003[^4], y está disponible en el Repositorio UCI[^5]. De las 495 características restantes, 15 corresponden a combinaciones lineales de las cinco relevantes, y las otras 480 son características de ruido. Madelon es un problema de clasificación de dos clases con variables de entrada binarias dispersas. Las dos clases están equilibradas, y los datos se dividen en conjuntos de entrenamiento y prueba.
    2.  *Leukemia*: El análisis de datos de expresión génica obtenidos de micro-datos de ADN se estudia en Golub [-@golubMolecularClassificationCancer1999] para la clasificación de tipos de cáncer. Construyeron un conjunto de datos con 7129 mediciones de expresión génica en las clases ALL (leucemia linfocítica aguda) y AML (leucemia mielogénica aguda). El problema es distinguir entre estas dos variantes de leucemia (ALL y AML). Los datos se dividen originalmente en dos subconjuntos: un conjunto de entrenamiento y un conjunto de prueba independiente. El conjunto de entrenamiento consta de 38 muestras (27 ALL y 11 AML) de especímenes de médula ósea. El conjunto de prueba tiene 34 muestras (20 ALL y 14 AML), preparadas bajo diferentes condiciones experimentales e incluyendo 24 especímenes de médula ósea y 10 muestras de sangre.
    3.  *GCM*: El conjunto de datos GCM fue compilado en Ramaswamy [-@ramaswamyMulticlassCancerDiagnosis2001] y contiene los perfiles de expresión de 198 muestras de tumores que representan 14 clases comunes de cáncer humano3. Aquí el enfoque estuvo en 190 muestras de tumores después de excluir 8 muestras de metástasis, y el preprocesamiento se realizó de acuerdo con \[24\]. Finalmente, cada matriz se estandarizó a una media de 0 y una varianza de 1 según \[25\]. El conjunto de datos consta de un total de 190 instancias, con 16063 atributos (biomarcadores) cada una, y distribuidos en 14 clases desequilibradas. En nuestros experimentos, los datos se separaron en tres conjuntos: entrenamiento (72 muestras), validación (72 muestras) y prueba (42 muestras). Con el objetivo de equilibrar las clases en el conjunto de entrenamiento y prevenir el sobreajuste del clasificador debido a la clase mayoritaria, las muestras se repitieron para obtener el mismo número para cada clase. Al final, se utilizaron un total de 168 muestras para el entrenamiento. Por el contrario, los conjuntos de validación y prueba están estratificados, manteniendo el desequilibrio de clases.
    4.  *Gisette:* es un dataset creado para trabajar el problema de reconocimiento de dígitos escritos a mano [@isabelleguyonGisette2004]. Este conjunto de datos forma parte de los cinco conjuntos utilizados en el desafío de selección de características de NIPS 2003. Tiene 13500 observaciones y 5000 atributos. El desafío radica en diferenciar los dígitos '4' y '9', que suelen ser fácilmente confundibles entre sí. Los dígitos han sido normalizados en tamaño y centrados en una imagen fija de 28x28 píxeles. Para el desafío de selección de características, se modificaron los datos originales. Específicamente, se muestrearon píxeles al azar en la parte superior central de la característica que contiene la información necesaria para diferenciar el 4 del 9. Además, se crearon características de orden superior como productos de estos píxeles para sumergir el problema en un espacio de características de mayor dimensión. También se añadieron características distractoras denominadas "sondas", que no tienen poder predictivo. El orden de las características y patrones fue aleatorizado.

[^4]: <http://clopinet.com/isabelle/Projects/NIPS2003/>

[^5]: <http://archive.ics.uci.edu/ml/datasets.html>

## Cronograma de trabajo

## Condiciones para el desarrollo

```{=html}
<!-- 

¿Qué presentar?
• Plan de tesis
• Compromiso y CV del director
• CV del Tesista
Recibe y hace una primera evaluación el Comité Académico. De tener el visto bueno se eleva para su evaluación en CS. Secciones del plan de Maestría según normativa:
Escribir a futuro. 

Extensión máxima recomendada: 8 páginas.

Título
• Debe expresar claramente el tema con rigurosidad y precisión técnica.

Fundamentación y justificación del tema (extensión máxima 2 páginas)
• Marco teórico.
• Valor científico del trabajo propuesto.
• Alcance.

Estado del arte (extensión máxima sugerida 2 páginas)
• Evolución histórica y actual del conocimiento.
• Aspectos o conocimiento que se encuentre vacantes. 
- Citar a mis directores!

Objetivos (extensión máxima sugerida 1 páginas) Preferentemente 1⁄2 página
• Enunciado claro de los objetivos que den fe de los alcances y límites.
• OG → línea de investigación
• OE → resultados esperados

Metodología y Actividades (extensión máxima sugerida 2 páginas) Lo que necesiten
- Seleccion de datos, como se prepararon los datos, estructura de los datos y Evaluación
- Implementación
• Actividades de desarrollar
• Metodologías técnicas.
Acá agregar cuestiones éticas en caso que utilicen datos de humanos (cómo los anonimizan, etc.).

Cronograma de trabajo (extensión máxima sugerida 2 páginas) → Preferentemente 1⁄2 o 1 página.
• Listar actividades con un diagrama de Gantt

Referencias bibliográficas (extensión máxima sugerida 2 páginas)→ Unas 20-25 como máximo.
• Actualizadas y pertinentes
• Elegir y respetar estilo (APA o IEEE)

Condiciones para el desarrollo (extensión máxima sugerida 1 páginas) → 1⁄2 página
• Mencionar y justificar la sede en la que se trabajará
• Equipamiento
• Software

-->
```
\pagebreak

### Bibliografía