---
title: "UTN Regional Paran√° - Maestr√≠a en Miner√≠a de Datos"
subtitle: "Plan de Tesis"
date: "Octubre 2023"
format:
  pdf:
    code-fold: false    
jupyter: python3
bibliography: references.bib
---

## Directores: Dr. Mat√≠as Gerard y Dr. Leandro Vignolo (CONICET-UNL)

### T√≠tulo: "Aumentaci√≥n de datos mediante autoencoders variacionales y su impacto en estrategias de selecci√≥n de caracter√≠sticas basadas en algoritmos gen√©ticos."

### **Alumno: Lic. Claudio Sebasti√°n Castillo (UTN)**

\pagebreak

## Fundamentaci√≥n y Justificaci√≥n del tema

Los algoritmos gen√©ticos (en adelante AG) son m√©todos de optimizaci√≥n inspirados en la evoluci√≥n natural, dise√±ados para encontrar soluciones en espacios de b√∫squeda complejos [@vignoloEvolutionaryLocalImprovement2017]. A diferencia de los m√©todos de optimizaci√≥n exhaustivos (ej. m√©todos enumerativos[^1]), los AG son particularmente efectivos en espacios de b√∫squeda discretos, ruidosos, cuando la funci√≥n objetivo no puede describirse mediante una ecuaci√≥n o la misma no es diferenciable [@goldbergdavide.GeneticAlgorithmsSearch1989]. Utilizando principios basados en la evoluci√≥n, estos algoritmos generan iterativamente soluciones a partir de una poblaci√≥n de candidatos, de manera similar a c√≥mo la evoluci√≥n natural optimiza caracter√≠sticas biol√≥gicas a lo largo de generaciones en funci√≥n de las condiciones del entorno. En contextos de aplicaci√≥n sus resultados regularmente conducen a soluciones cercanas al √≥ptimo, capaces de mantener un buen compromiso en la satisfacci√≥n de m√∫ltiples requerimientos [@jiaoSurveyEvolutionaryMultiobjective2023]. Por eso, los AG son eficaces para atacar tanto problemas de objetivo √∫nico, como problemas multiobjetivo.

[^1]: @goldbergdavide.GeneticAlgorithmsSearch1989, p.4.

La robustez de los AG est√° determinada, como bien sostiene Goldberg [-@goldbergdavide.GeneticAlgorithmsSearch1989], por una serie de caracter√≠sticas distintivas, que fortalecen su configuraci√≥n de b√∫squeda, a saber: a) operan sobre un espacio *codificado* del problema y no sobre el espacio en su representaci√≥n original; b) realizan la exploraci√≥n evaluando una *poblaci√≥n de soluciones* y no soluciones individuales; c) tienen como gu√≠a una *funci√≥n objetivo* (tambi√©n llamada *funci√≥n de aptitud*) que no requiere derivaci√≥n u otras funciones de c√°lculo; y d) suponen *m√©todos probabil√≠sticos de transici√≥n* (operadores estoc√°sticos) y no reglas determin√≠sticas. Estas caracter√≠sticas permiten a los AG superar restricciones que tienen otros m√©todos de optimizaci√≥n, condicionados -por ejemplo- a espacios de b√∫squeda continuos, diferenciables o unimodales. Por ello, su aplicaci√≥n se ha difundido notablemente, trascendiendo los problemas cl√°sicos de optimizaci√≥n, aplic√°ndose en distintas tareas [@vieQualitiesChallengesFuture2021] y a lo largo de diversas industrias [@jiaoSurveyEvolutionaryMultiobjective2023].

La importancia de los AGs como herramientas de optimizaci√≥n, adquiere especial preeminencia en el problema de *selecci√≥n de caracter√≠sticas* [@jiaoSurveyEvolutionaryMultiobjective2023], por lo que en este trabajo dirigiremos la atenci√≥n en esa direcci√≥n*.* La *selecci√≥n de caracter√≠sticas* (en adelante SC) representa un desaf√≠o de optimizaci√≥n combinatoria complejo, que despierta inter√©s en el universo del aprendizaje autom√°tico debido a su impacto en el rendimiento de los modelos y la posibilidad de reducir la complejidad computacional de ciertos problemas. Tal desaf√≠o est√° determinado por varios factores. En primer lugar encontramos que, en espacios de alta dimensionalidad, la cardinalidad del conjunto de soluciones candidatas crece de manera exponencial, y los problemas se vuelven computacionalmente intratables debido a la extensi√≥n del espacio de b√∫squeda.[^2] En segundo lugar, junto con la alta dimensionalidad, aparece el problema de las interacciones entre caracter√≠sticas. Aqu√≠, el prol√≠fico espectro de dependencias que pueden establecer los atributos plantea normalmente v√≠nculos dif√≠ciles de modelar atento a que se multiplican de la mano de la dimensionalidad.[^3] Por √∫ltimo, aunque no por ello menos importante, aparece el car√°cter multiobjetivo de los problema de SC, donde no solo interesa maximizar la eficacia de los modelos sino tambi√©n que sean eficientes. Eficiencia que implica -generalmente- la necesidad de minimizar la cantidad de atributos seleccionados para resolver un problema [@jiaoSurveyEvolutionaryMultiobjective2023].

[^2]: Cabe destacar que para un conjunto de `n` caracter√≠sticas es posible determinar un total de `n2` posibles soluciones, espacio que constituye un dominio de b√∫squeda dif√≠cil de cubrir a√∫n con `n` conservadores. Por ejemplo para un conjunto de 20 caracter√≠sticas (atributos) el n√∫mero total de subconjuntos a evaluar supera el mill√≥n de posibles candidatos, espec√≠ficamente: 1.048.576.

[^3]: Por ejemplo, dos caracter√≠sticas con alto valor discriminatorio para resolver un problema de clasificaci√≥n pueden ser redundantes debido a su correlaci√≥n y exigir criterios inteligentes de inclusi√≥n-exclusi√≥n. A la inversa, caracter√≠sticas que individualmente consideradas pueden carecer de valor discriminatorio, debido a su complementariedad pueden ser esenciales para resolver un problema y por lo tanto exigir criterios complejos de evaluaci√≥n y b√∫squeda.

Estos desaf√≠os son abordados por los AGs de manera conveniente y creativa.[^4] En el marco de este algoritmo cada individuo (muestra) representa una soluci√≥n candidata, con un perfil gen√©tico particular determinado por un subconjunto de caracter√≠sticas. La b√∫squeda de las mejores soluciones comienza con la selecci√≥n de una poblaci√≥n inicial de individuos y un subconjunto de caracter√≠sticas generados aleatoriamente. Este subconjunto se eval√∫a utilizando una funci√≥n de aptitud, y los individuos con mejor rendimiento (puntaje) son seleccionados para la reproducci√≥n. Este proceso contin√∫a durante un cierto n√∫mero de generaciones hasta que se cumple una condici√≥n de terminaci√≥n [@goldbergdavide.GeneticAlgorithmsSearch1989].

[^4]: Ciertamente, no son sus atributos aislados los que le dan esa posibilidad, sino la interacci√≥n de sus componentes.

Este mecanismo simple constituye un eficaz m√©todo de selecci√≥n en contextos de alta dimensionalidad y bajo n√∫mero de muestras. Esa eficacia se debe a la capacidad de explorar el problema dividi√©ndolo en subespacios de caracter√≠sticas y, al mismo tiempo, explotar las regiones de mayor valor en cada subespacio [@goldbergdavide.GeneticAlgorithmsSearch1989].[^5]

[^5]: Ambas funciones -exploraci√≥n y explotaci√≥n- permiten al algoritmo reconfigurar el espacio de b√∫squeda y poner a prueba sus complejas dependencias. Como vimos, el procedimiento es orientado por una funci√≥n de aptitud que eval√∫a las distintas posibilidades combinatorias encontradas por el algoritmo y retroalimenta el proceso exploratorio. La din√°mica completa tiene como resultado un procedimiento experimental de b√∫squeda y selecci√≥n capaz de reconocer soluciones pr√≥ximas al √≥ptimo.

Dicho lo anterior, no es menos cierto que la capacidad de selecci√≥n de los AGs depende de la evaluaci√≥n de aptitud que orienta la b√∫squeda de las mejores soluciones, y tal evaluaci√≥n descansa -finalmente- en la disponibilidad de datos. En efecto, la existencia y n√∫mero de individuos condiciona la funci√≥n objetivo y por esa v√≠a tambi√©n al proceso de selecci√≥n de caracter√≠sticas de los AGs. La disponibilidad de datos resulta as√≠ un factor clave para la selecci√≥n. Este requerimiento, vinculado particularmente a la funci√≥n objetivo, se presenta no solo cuando se utiliza como evaluador a modelos complejo de aprendizaje autom√°tico (que demandan una cantidad creciente de muestras de entrenamiento)[^6], sino tambi√©n cuando se trabaja sobre datos cuyas clases se encuentran desbalanceadas [@fajardoOversamplingImbalancedData2021; @blagusSMOTEHighdimensionalClassimbalanced2013]. En ambos escenarios, la falta de informaci√≥n suficiente degrada la capacidad informativa de la funci√≥n objetivo [@hastieElementStatisticalLearning2009], afectando gravemente el proceso de selecci√≥n de caracter√≠sticas.

[^6]: @alzubaidiSurveyDeepLearning2023.

En esa l√≠nea, el problema de la disponibilidad de datos en los proyecto de selecci√≥n de caracter√≠sticas -sea dentro o fuera del campo de los AGs-, ha encontrado en las estrategias de aumentaci√≥n una posible soluci√≥n [@gmComprehensiveSurveyAnalysis2020]. Entre esas estrategias, los Autoencoders Variacionales (en adelante AV) han adquirido popularidad, superando a m√©todos tradicionales (ej. sobremuestreo [@blagusSMOTEHighdimensionalClassimbalanced2013]) y -en ciertos casos- tambi√©n a otro modelos generativos basados de redes neuronales profundas [@fajardoOversamplingImbalancedData2021].

Los AVs constituyen modelos generativos[^7] capaces de aprender una representaci√≥n latente de datos observados y producir nuevas muestras con las mismas caracter√≠sticas fundamentales[^8] que las observaciones [@kingmaIntroductionVariationalAutoencoders2019]. Esa capacidad resulta particularmente efectiva por el hecho de que prescinde de fuertes supuestos estad√≠sticos a los que adscriben otros modelos generativos y tambi√©n por su escalabilidad.[^9] Hoy los AVs son ampliamente utilizados en biolog√≠a molecular, qu√≠mica, procesamiento de lenguaje natural, astronom√≠a, entre otros [@ramchandranLearningConditionalVariational2022].

[^7]: Redes neuronales profundas con arquitectura *encoder-decoder* [@kingmaIntroductionVariationalAutoencoders2019]. Estos modelos pueden presentar distintas configuraciones seg√∫n el problema tratado y el objetivo particular de la implementaci√≥n [@wuEVAEEvolutionaryVariational2023].

[^8]: Similar distribuci√≥n conjunta de probabilidad.

[^9]: El modelo emplea *retropropagaci√≥n* como estrategia de optimizaci√≥n [@kingmaIntroductionVariationalAutoencoders2019]

Por todo lo visto hasta aqu√≠ advertimos que la posibilidad de expandir el conjunto de datos mediante el uso de AVs abre nuevas alternativas para afrontar el problema de la selecci√≥n de caracter√≠sticas aplicando AGs. Estas alternativas no solo parecen prometedoras como estrategias orientadas a la multiplicaci√≥n de muestras de entrenamiento para mejorar el desempe√±o de la funci√≥n objetivo, sino tambi√©n como partes funcionales de sus operadores de variaci√≥n.[^10] De este modo, la integraci√≥n de ambas tecnolog√≠as ofrece un enfoque provechoso para abordar el problema de selecci√≥n de caracter√≠sticas en distintos escenarios que enfrentan los AGs.

[^10]: As√≠, la integraci√≥n de los AVs en el contexto de los AGs podr√≠a dirigirse no solo a la multiplicaci√≥n general de datos, sino tambi√©n a la multiplicaci√≥n selectiva de ciertos subconjuntos de caracter√≠sticas valiosas

A la fecha de publicaci√≥n del presente trabajo no hemos encontrado experiencias de aplicaci√≥n de AVs en el √°mbito de selecci√≥n de caracter√≠sticas mediante AGs. En la medida que esto sea as√≠ creemos que nuestro aporte a la comunidad de investigadores y practicantes de la disciplina estar√° en proveer informaci√≥n y experimentaci√≥n sobre la combinaci√≥n de ambos algoritmos. Dicho aporte tendr√≠a un alcance nacional a todos aquellos equipos dedicados al problema de selecci√≥n de caracter√≠sticas aplicando computaci√≥n evolutiva.

## Estado del arte

La aplicaci√≥n de AVs como t√©cnica de aumentaci√≥n de datos es extendida fuera del campo de los AGs. Se aplica al tratamiento varios tipos de datos: im√°genes [@fajardoOversamplingImbalancedData2021; @aiGenerativeOversamplingImbalanced2023; @khmaissiaConfidenceGuidedDataAugmentation2023; @kwarciakDeepGenerativeNetworks2023], texto [@zhangImproveDiverseText2019] , habla [@blaauwModelingTransformingSpeech2016; @latifVariationalAutoencodersLearning2020] y m√∫sica [@robertsHierarchicalLatentVector2019], y distintos formatos: tabulares [@leelarathnaEnhancingRepresentationLearning2023], longitudinales [@ramchandranLearningConditionalVariational2022] y grafos [@liuConstrainedGraphVariational2018]. En lo que sigue nos enfocaremos en el uso de AVs en el marco del problema de selecci√≥n de caracter√≠sticas. 

As√≠ en @fajardoOversamplingImbalancedData2021 se investiga si los AVs y las redes generativas antag√≥nicas (GAN) pueden aumentar datos desbalanceados via sobremuestreo de las clases minoritarias, y luego mejorar el rendimiento de un clasificador. Para ello se crean versionanes desbalanceadas de reconocidos datos multiclase MNIST y Fashion MNIST, a los cuales, posteriormente, se los re-balancea agreg√°ndoles muestras sint√©ticas generadas por un AV condicionado por clase (AV Condicional). Para la tarea de clasificaci√≥n se emplea un Perceptr√≥n Multicapa (MLP) y se eval√∫a su desempe√±o a partir de  F1....

adem√°s se eval√∫a la calidad de los datos generados mediante Distancia de inicio de Fr√©chet




Para ambos conjuntos de datos, MNIST y Fashion MNIST, empleamos un perceptr√≥n multicapa sencillo (MLP) para la clasificaci√≥n subsecuente. Como es habitual en los MLPs y otras arquitecturas de redes neuronales, los pesos de la red se inicializan aleatoriamente antes del entrenamiento. Como resultado, existe cierta cantidad de aleatoriedad en el rendimiento de la clasificaci√≥n subsecuente. Por lo tanto, para cada uno de los 30 conjuntos de entrenamiento artificialmente desequilibrados de MNIST y Fashion MNIST, aumentados por un m√©todo de sobremuestreo dado, se entrenan tres modelos MLP subsecuentes separados. Esto implica que cada uno de los cinco conjuntos de entrenamiento asociados con el tr√≠o (ùê¶, ùêå, ùúå) tienen tres colecciones de m√©tricas de rendimiento del conjunto de prueba. Cada una de estas m√©tricas se promedia en todas las 15 instancias para producir un √∫nico conjunto de medidas de rendimiento asociadas con un tr√≠o dado (ùê¶, ùêå, ùúå).




 Los resultados indican que efectivamente los modelos generativos profundos superan a los m√©todos tradicionales (ej. sobremuestreo) en problemas de clasificaci√≥n, especialmente en casos de desequilibrio severo.

En @aiGenerativeOversamplingImbalanced2023 se ataca el problema de datos desbalanceados y se propone una forma de aumentaci√≥n de datos en la clase minoritaria que condiciona la generaci√≥n de muestras a las caracter√≠sticas de la distribuci√≥n que tienen los datos de la clase mayoritaria. El m√©todo se llama *Majority-Guided VAE* y procura incorporar en la generaci√≥n informaci√≥n intraclase e interclases, con el fin de propagar la diversidad y riqueza de la mayor√≠a en la minor√≠a, y mitigar as√≠ el sobreajuste de los modelos.

En @khmaissiaConfidenceGuidedDataAugmentation2023 se emplean AVs para aumentaci√≥n de datos en una tarea de clasificaci√≥n de im√°genes con enfoque semi-supervisado. Primero, entrenamos un modelo CNN b√°sico. Luego, identificamos regiones desafiantes en el espacio de caracter√≠sticas al encontrar todas las muestras mal clasificadas y las muestras correctamente clasificadas con valores de confianza bajos. Estas muestras se utilizan para entrenar un Autoencoder Variacional (VAE). A continuaci√≥n, el VAE se utiliza para generar im√°genes sint√©ticas. Finalmente, estas im√°genes sint√©ticas se usan junto con las im√°genes originales etiquetadas para entrenar un nuevo modelo de manera semi-supervisada. Los resultados emp√≠ricos en conjuntos de datos de referencia como STL10 y CIFAR-100 muestran que las muestras sint√©ticamente generadas pueden diversificar a√∫n m√°s los datos de entrenamiento, lo que lleva a una mejora en la clasificaci√≥n de im√°genes en comparaci√≥n con los enfoques de l√≠nea de base completamente supervisados que utilizan solo los datos disponibles.

En @leelarathnaEnhancingRepresentationLearning2023 se emplean combinaciones (`ensembles`) de AVs para atacar el problema de la reducci√≥n de dimensionalidad ante datos de alta dimensionalidad y bajo n√∫mero de muestras. A trav√©s de una serie de experimentos en ocho conjuntos de datos del mundo real, se demuestra que el m√©todo aprende mejores representaciones latentes, y conduce a una mayor precisi√≥n en una tarea de clasificaci√≥n.

**AV en el contexto del AG**

En @martinsVariationalAutoencodersEvolutionary2022b .... Los desarrollos recientes en el aprendizaje profundo generativo han fomentado nuevos m√©todos de ingenier√≠a para el dise√±o de prote√≠nas. Aunque los modelos generativos profundos entrenados en secuencias de prote√≠nas pueden aprender representaciones biol√≥gicamente significativas, el dise√±o de prote√≠nas con propiedades optimizadas sigue siendo un desaf√≠o. Combinamos arquitecturas de aprendizaje profundo con computaci√≥n evolutiva para dirigir el proceso generativo de prote√≠nas hacia conjuntos espec√≠ficos de propiedades para abordar este problema. El espacio latente de un Autoencoder Variacional se explora mediante algoritmos evolutivos para encontrar los mejores candidatos. Se concibi√≥ un conjunto de problemas de un solo objetivo y de m√∫ltiples objetivos para evaluar la capacidad de los algoritmos para optimizar prote√≠nas. Las tareas de optimizaci√≥n consideran la hidrofobicidad promedio de las prote√≠nas, su solubilidad y la probabilidad de ser generadas por un perfil de Modelo Oculto de Markov funcional definido. Los resultados muestran que los Algoritmos Evolutivos pueden lograr buenos resultados mientras permiten m√°s variabilidad en el dise√±o del experimento, lo que resulta en un conjunto mucho mayor de prote√≠nas novedosas posiblemente funcionales.

**AVEvolutivo**

En @wuEVAEEvolutionaryVariational2023 Desde la perspectiva del cuello de botella de informaci√≥n, los VAE (Autoencoders Variacionales) pueden tratarse como un proceso de compresi√≥n de informaci√≥n con p√©rdida. Ajustar la divergencia KL dentro de un rango controla el cuello de botella de informaci√≥n que fluye desde la representaci√≥n de variables latentes hasta la reconstrucci√≥n de muestras, beneficiando as√≠ el equilibrio entre compresi√≥n y reconstrucci√≥n (Alemi et al. 2017; Burgess et al. 2018). Este cuello de botella de informaci√≥n variacional nos inspira para mejorar el mecanismo y la funci√≥n objetivo del VAE. Para abordar directamente el equilibrio entre generaci√≥n e inferencia en un entorno din√°mico en evoluci√≥n sin restricciones en redes y entradas, presentamos un nuevo VAE evolutivo (eVAE). El eVAE incorpora e integra el aprendizaje evolutivo variacional y el cuello de botella de informaci√≥n variacional en el VAE para lograr una mejor exploraci√≥n √≥ptima y el equilibrio entre la compresi√≥n de la representaci√≥n y el ajuste de la generaci√≥n. Sin embargo, integrar el aprendizaje evolutivo en el VAE es un tema abierto. Proponemos el algoritmo gen√©tico variacional en el eVAE para optimizar los objetivos del VAE y su exploraci√≥n de modelado de manera evolutiva. En consecuencia, el eVAE optimiza din√°micamente la inferencia del VAE (los t√©rminos KL) de manera evolutiva y probabil√≠stica, lo que ajusta a√∫n m√°s la generaci√≥n del VAE hacia el equilibrio entre generaci√≥n e inferencia.

## Definici√≥n del problema

La disponibilidad de datos muestrales afecta el proceso de selecci√≥n de caracter√≠sticas aplicando AGs debido a su impacto en la funci√≥n objetivo. Este impacto es particularmente negativo en escenarios de alta dimensionalidad y bajo n√∫mero de muestras. Por eso, la t√©cnica de aumentaci√≥n de datos mediante AVs plantea una posible soluci√≥n a este problema, ofreciendo distintas alternativas de implementaci√≥n en el contexto de los AGs.

## Objetivos

**General**:

Evaluar el impacto de la aumentaci√≥n de datos mediante autoencoders variacionales sobre el desempe√±o de estrategias de selecci√≥n de caracter√≠sticas basados en algoritmos gen√©ticos.

**Espec√≠ficos**:

1.  Implemetar un algoritmo gen√©tico para selecci√≥n de caracter√≠sticas.

2.  Integrar estrategias de aumentaci√≥n de datos basadas en AVs en los AGs.

3.  Evaluar el desempe√±o de las estrategias propuestas empleando conjuntos de datos de diferente complejidad.

4.  Comparar el desempe√±o de AGs con datos aumentados mediante las estrategias desarrolladas frente a implementaciones sin aumentaci√≥n.

## Metodolog√≠as

Para cumplir los objetivos propuestos compararemos el desempe√±o de un algoritmo gen√©tico *cl√°sico* frente a uno que incorpore las estrategias de aumentaci√≥n que se desarrollen. Por un lado implementaremos un algoritmo gen√©tico *cl√°sico* aplicado a conjuntos de datos conocidos dentro de la comunidad cient√≠fica y en condiciones regulares de procesamiento para la selecci√≥n de caracter√≠sticas en problemas de aprendizaje autom√°tico. Por el otro, aplicaremos a los mismos conjuntos un variante *novedosa* de algoritmo gen√©tico, integrando un m√≥dulo de aumentaci√≥n de datos a partir de la intervenci√≥n de AVs. Para evaluar esta variante de algoritmo gen√©tico realizaremos diversos experimentos con ajustes en el dise√±o de la arquitectura que consistir√°n en la integraci√≥n del m√≥dulo de aumentaci√≥n de datos en dos puntos: 1) previo al inicio del proceso evolutivo y 2) durante el proceso evolutivo del algoritmo gen√©tico. Para la comparaci√≥n de ambas implementaciones elegiremos distintas m√©tricas de eficacia en la clasificaci√≥n, teniendo en cuenta en los resultados el tama√±o final de caracter√≠sticas seleccionadas.

A continuaci√≥n detallamos los aspectos t√©cnicos vinculados a los elementos mencionados, a saber: algoritmos, datos, arquitecturas y m√©tricas para la evaluaci√≥n.

### Algoritmos

Para el presente trabajo usaremos algoritmos gen√©ticos (AGs) como m√©todo de b√∫squeda[^11] debido a la posibilidad que brindan de emplear codificaci√≥n binaria y permitir as√≠ una representaci√≥n intuitiva del espacio de caracter√≠sticas [@vignoloEvolutionaryLocalImprovement2017]. Para aumentaci√≥n de datos utilizaremos *autoencoders variacionales* (AVs) como instancia generativa [@kingmaIntroductionVariationalAutoencoders2019].

[^11]: Otros m√©todos robustos, como por ejemplo el *enjambre de part√≠culas* (PSO) y *optimizaci√≥n de colonia de hormigas* (ACO), t√≠picamente utilizan codificaci√≥n basada en n√∫meros reales por lo que constituyen opciones menos adecuadas al problema que enfrentaremos en este trabajo.

Los AGs constituyen una de las herramientas m√°s estudiadas e implementadas dentro de los m√©todos evolutivos [@goldbergdavide.GeneticAlgorithmsSearch1989, @kramerGeneticAlgorithmEssentials2017], dada su capacidad para encontrar soluciones en espacios de b√∫squeda complejos [@vignoloEvolutionaryLocalImprovement2017]. El procedimiento de b√∫squeda de los AGs opera evolucionando una poblaci√≥n de individuos que consisten en cromosomas que codifican el espacio de soluciones. Dicha evoluci√≥n -al igual que la evoluci√≥n natural- sucede a trav√©s de operadores (funciones) de selecci√≥n, variaci√≥n (mutaci√≥n y cruce) y reemplazo que transforman el material gen√©tico disponible: los individuos m√°s aptos sobreviven y se reproducen, mientras que los menos aptos desaparecen[^12]. Esta aptitud -que imita la presi√≥n selectiva de un entorno natural- se eval√∫a mediante la aplicaci√≥n de una funci√≥n objetivo (espec√≠fica del problema) a cada individuo a partir de la informaci√≥n decodificada de sus cromosomas. Dicha funci√≥n objetivo puede asumir m√∫ltiples formas [@jiaoSurveyEvolutionaryMultiobjective2023], pero en nuestro trabajo nos centraremos en el uso de modelos de aprendizaje autom√°tico, particularmente Maquinas de Soporte Vectorial [@boserTrainingAlgorithmOptimal1992] y Bosques Aleatorios [@breimanRandomForests2001]. Este m√©todo heur√≠stico de b√∫squeda tendr√° en nuestro trabajo dos configuraciones: una *cl√°sica* sin aumentaci√≥n de datos y una *novedosa* con aumentaci√≥n de datos aplicando *autoencoders variacionales* (AV)*.*

[^12]: Como su nombre lo indica el operador de selecci√≥n determina la elegibilidad de un individuo para sobrevivir y reproducirse en funci√≥n de su aptitud para resolver un problema. En el contexto de los AGs esta aptitud no es otra cosa que el puntaje que obtiene un individuo evaluado en una funci√≥n objetivo. Por su parte los operadores de variaci√≥n tienen como funci√≥n combinar la informaci√≥n gen√©tica de individuos (cruce) y alterar aleatoriamente sus cromosomas (mutaci√≥n), promoviendo transformaciones en el material gen√©tico global con sesgo hacia mejorar la aptitud poblacional para resolver un problema. La variaci√≥n equivale a la b√∫squeda natural por mejorar las adaptaciones de los individuos a su entorno. Finalmente el operador de reemplazo mantiene la poblaci√≥n constante, sustituyendo individuos poco aptos por aquellos de mayor aptitud. Estos operadores se combinan en ciclos iterativos que se repiten hasta satisfacer un criterio de terminaci√≥n deseado (por ejemplo, un n√∫mero predefinido de generaciones o un valor de aptitud) [@vignoloEvolutionaryLocalImprovement2017].

Para aumentar el conjunto de datos que emplear√° la funci√≥n de aptitud de los AGs emplearemos *autoencoders variacionales* (AVs). Los AVs son modelos generativos implementados por redes neuronales profundas con arquitectura *encoder-decoder* capaces de aprender una representaci√≥n latente de datos disponibles y generar nuevas muestras de similares caracter√≠sticas a los datos originales [@kingmaIntroductionVariationalAutoencoders2019]. Estos modelos se basan en el supuesto de que cualquier dato disponible, por ejemplo $x$, se genera mediante un proceso aleatorio que involucra una variable latente $z$. Bajo ese supuesto, el modelo procede tomando como muestra una observaci√≥n de $z$ de la distribuci√≥n de probabilidad *a priori* $p_\theta(z)$, que luego se utiliza para tomar una observaci√≥n de $x$ de la distribuci√≥n condicional $p_\theta(x|z)$. El objetivo del modelo es obtener *estimaciones de m√°xima verosimilitud* del par√°metro $\theta$ en situaciones donde tanto la verosimilitud marginal $p_\theta(x) = \int p_\theta(z)p_\theta(x|z) dz$ como la probabilidad *a posteriori* $p_\theta(x|z)$ son intratables[^13]. Para eso, utiliza la distribuci√≥n $q_\phi(z|x)$ como una aproximaci√≥n al intratable $p_\theta(x|z)$, maximizando el *l√≠mite inferior variacional*[^14] para $p_\theta(x)$. El objetivo de aprendizaje del AV se da entonces por:

[^13]: Son intratables porque $z$ es una variable latente, no observada, y el c√≥mputo de probabilidad que la incluya -en este caso $x$ - debe *marginalizar* (integrar) todo sus posibles valores, situaci√≥n computacionalmente costosa en el contexto del modelos analizado.

[^14]: Limite obtenido a trav√©s de una funci√≥n auxiliar conocida como funci√≥n *ELBO.*

> $\mathcal{L}_{AV}(x; \theta, \phi) = \max(\phi,\theta) \left( E_{z \sim q_\phi(z|x)} [\log p_\theta (x|z)] - \text{KL}(q_\phi(z|x) \| p_\theta (z)) \right),$

donde $\text{KL}(q(\cdot) \| p(\cdot))$ denota la divergencia de Kullback--Liebler entre dos distribuciones $q(\cdot)$ y $p(\cdot)$. Una vez que el AV est√° entrenado, una observaci√≥n sint√©tica $x'$ se genera tomando primero $z \sim p_\theta(z)$ y posteriormente tomando $x'$ de la probabil√≠stica condicional entrenada por el modelo $p_\theta(x|z)$.

### **Conjuntos de Datos**

Se llevar√°n a cabo experimentos utilizando distintos conjuntos de datos con alta dimensionalidad y diferente n√∫mero de caracter√≠sticas, incluyendo diversos n√∫meros de clases. En principio proponemos los siguientes datos:

1.  *Madelon*: Este es un conjunto de datos artificial con 500 caracter√≠sticas, donde el objetivo es un XOR multidimensional con cinco caracter√≠sticas relevantes. Fue creado para el desaf√≠o de Selecci√≥n de Caracter√≠sticas NIPS 2003[^15], y est√° disponible en el Repositorio UCI[^16]. De las 495 caracter√≠sticas restantes, 15 corresponden a combinaciones lineales de las cinco relevantes, y las otras 480 son caracter√≠sticas de ruido. Madelon es un problema de clasificaci√≥n de dos clases con variables de entrada binarias dispersas. Las dos clases est√°n equilibradas, y los datos se dividen en conjuntos de entrenamiento y prueba.
2.  *Leukemia*: El an√°lisis de datos de expresi√≥n g√©nica obtenidos de micro-datos de ADN se estudia en Golub [-@golubMolecularClassificationCancer1999] para la clasificaci√≥n de tipos de c√°ncer. Construyeron un conjunto de datos con 7129 mediciones de expresi√≥n g√©nica en las clases ALL (leucemia linfoc√≠tica aguda) y AML (leucemia mielog√©nica aguda). El problema es distinguir entre estas dos variantes de leucemia (ALL y AML). Los datos se dividen originalmente en dos subconjuntos: un conjunto de entrenamiento y un conjunto de prueba independiente. El conjunto de entrenamiento consta de 38 muestras (27 ALL y 11 AML) de espec√≠menes de m√©dula √≥sea. El conjunto de prueba tiene 34 muestras (20 ALL y 14 AML), preparadas bajo diferentes condiciones experimentales e incluyendo 24 espec√≠menes de m√©dula √≥sea y 10 muestras de sangre.
3.  *GCM*: El conjunto de datos GCM fue compilado en Ramaswamy [-@ramaswamyMulticlassCancerDiagnosis2001] y contiene los perfiles de expresi√≥n de 198 muestras de tumores que representan 14 clases comunes de c√°ncer humano3. Aqu√≠ el enfoque estuvo en 190 muestras de tumores despu√©s de excluir 8 muestras de met√°stasis, y el preprocesamiento se realiz√≥ de acuerdo con \[24\]. Finalmente, cada matriz se estandariz√≥ a una media de 0 y una varianza de 1 seg√∫n \[25\]. El conjunto de datos consta de un total de 190 instancias, con 16063 atributos (biomarcadores) cada una, y distribuidos en 14 clases desequilibradas. En nuestros experimentos, los datos se separaron en tres conjuntos: entrenamiento (72 muestras), validaci√≥n (72 muestras) y prueba (42 muestras). Con el objetivo de equilibrar las clases en el conjunto de entrenamiento y prevenir el sobreajuste del clasificador debido a la clase mayoritaria, las muestras se repitieron para obtener el mismo n√∫mero para cada clase. Al final, se utilizaron un total de 168 muestras para el entrenamiento. Por el contrario, los conjuntos de validaci√≥n y prueba est√°n estratificados, manteniendo el desequilibrio de clases.
4.  *Gisette:* es un dataset creado para trabajar el problema de reconocimiento de d√≠gitos escritos a mano [@isabelleguyonGisette2004]. Este conjunto de datos forma parte de los cinco conjuntos utilizados en el desaf√≠o de selecci√≥n de caracter√≠sticas de NIPS 2003. Tiene 13500 observaciones y 5000 atributos. El desaf√≠o radica en diferenciar los d√≠gitos '4' y '9', que suelen ser f√°cilmente confundibles entre s√≠. Los d√≠gitos han sido normalizados en tama√±o y centrados en una imagen fija de 28x28 p√≠xeles. Para el desaf√≠o de selecci√≥n de caracter√≠sticas, se modificaron los datos originales. Espec√≠ficamente, se muestrearon p√≠xeles al azar en la parte superior central de la caracter√≠stica que contiene la informaci√≥n necesaria para diferenciar el 4 del 9. Adem√°s, se crearon caracter√≠sticas de orden superior como productos de estos p√≠xeles para sumergir el problema en un espacio de caracter√≠sticas de mayor dimensi√≥n. Tambi√©n se a√±adieron caracter√≠sticas distractoras denominadas "sondas", que no tienen poder predictivo. El orden de las caracter√≠sticas y patrones fue aleatorizado.

[^15]: <http://clopinet.com/isabelle/Projects/NIPS2003/>

[^16]: <http://archive.ics.uci.edu/ml/datasets.html>

Dejamos planteada la posibilidad de introducir nuevos conjuntos de datos en funci√≥n de los resultados obtenidos y la necesidad de profundizar nuestros experimentos.

### **Arquitecturas**

Para nuestro trabajo utilizaremos dos configuraciones para nuestro AG: una *cl√°sica* integrada por un AG, cuya funci√≥n objetivo estar√° conformada por un modelo de Bosques Aleatorios sin aumentaci√≥n de datos y una *novedosa* con una estructura an√°logo de algoritmos pero con la intoducci√≥n de un m√≥dulo de aumentaci√≥n de datos aplicando *autoencoders variacionales* (AV). A su vez, dentro de esta segunda configuraci√≥n ensayaremos ajustes en el dise√±o que consistir√°n en la integraci√≥n del m√≥dulo de aumentaci√≥n de datos en dos puntos: 1) previo al inicio del proceso evolutivo y 2) durante el proceso evolutivo. De esta forma buscamos evaluar el aporte generativo del modelo AV en distintas etapas del AG.

### **M√©tricas de evaluaci√≥n**

Para la comparaci√≥n de ambas implementaciones elegiremos como m√©tricas UAR, F1 y el coeficiente de correlaci√≥n de Matthews (MCC), todos ellas en relaci√≥n al tama√±o final de caracter√≠sticas seleccionadas por cada modelo. Generalmente los AGs constituye modelos *multiobjetivo* donde no solo interesa optimizar una m√©trica particular vinculada a su eficacia -por ejemplo precisi√≥n en su clasificaci√≥n de individuos- sino tambi√©n interesa satisfacer requerimientos de eficiencia -como por ejemplo minimizar el conjunto de caracter√≠sticas con que opera el modelo- [@jiaoSurveyEvolutionaryMultiobjective2023]. Por esta raz√≥n nuestra evaluaci√≥n tomar√° en cuenta ambas dimensiones en el desempe√±o de las soluciones propuestas.

## Condiciones para el desarrollo

En el aspecto t√©cnico, el proyecto se dividir√° en dos fases en cuanto a la infraestructura. La primera fase ser√° la del desarrollo y depuraci√≥n de c√≥digo, que se llevar√° a cabo en una laptop personal. Este equipo cuenta con 12 GB de memoria RAM y un procesador Intel Core i7 de 4 n√∫cleos, lo cual es suficiente para programar, probar algoritmos y analizar datos a peque√±a escala. Sin embargo, debido a la complejidad y el volumen de los datos que se manejar√°n, esta configuraci√≥n local podr√≠a resultar insuficiente para los experimentos a gran escala. Por lo tanto, la segunda fase de experimentaci√≥n se realizar√° en la nube, utilizando Google Cloud Compute (GCC). Estas m√°quinas en la nube nos permitir√° configurar recursos en torno a 30 GB de memoria RAM y 12 n√∫cleos de CPU, posibilitando as√≠ manejar grandes vol√∫menes de datos y realizar c√°lculos complejos de manera m√°s eficiente. Este enfoque dual asegura un desarrollo √°gil a la vez que permite experimentos computacionalmente intensivos.

Respecto a software el proyecto emplear√° Python 3.9. A continuaci√≥n se detallan las librer√≠as de Python que ser√°n utilizadas para el desarrollo, los conjuntos de datos en los que se llevar√°n a cabo los experimentos y las arquitecturas computacionales que se explorar√°n en el estudio. Este detalle no es excluyente, pudiendo ampliarse en pos del cumplimiento de los objetivos propuestos.

### Librer√≠as de Python

-   `numpy`: Para manipulaci√≥n de matrices y c√°lculos matem√°ticos.
    -   Repositorio: [GitHub - numpy](https://github.com/numpy/numpy)
-   `pandas`: Para manejo de conjuntos de datos.
    -   Repositorio: [GitHub - pandas](https://github.com/pandas-dev/pandas)
-   `scikit-learn`: Para algoritmos de aprendizaje autom√°tico y preprocesamiento de datos.
    -   Repositorio: [GitHub - scikit-learn](https://github.com/scikit-learn/scikit-learn)
-   `lightgbm`: Para implementar el modelo de Bosques Aleatorios.
    -   Repositorio: [GitHub - LightGBM](https://github.com/microsoft/LightGBM)
-   `DEAP`: Para implementar algoritmos gen√©ticos.
    -   Repositorio: [GitHub - DEAP](https://github.com/DEAP/deap)
-   `pytorch`: Para implementaciones adicionales que requieran esta librer√≠a de aprendizaje profundo.
    -   Repositorio: [GitHub - PyTorch](https://github.com/pytorch/pytorch)
-   `MLflow`: Para el seguimiento y la gesti√≥n del ciclo de vida del modelo de aprendizaje autom√°tico.
    -   Repositorio: [GitHub - MLflow](https://github.com/mlflow/mlflow)

### Conjuntos de Datos

1.  **Madelon**
    -   Fuente: [UCI Repository](https://archive.ics.uci.edu/ml/datasets/Madelon)
2.  **Leukemia**
    -   Fuente: Art√≠culo @golubMolecularClassificationCancer1999. El conjunto de datos puede encontrarse en varios repositorios de investigaci√≥n en biolog√≠a computacional.
3.  **GCM**
    -   Fuente: Art√≠culo @ramaswamyMulticlassCancerDiagnosis2001. Disponible en repositorios de biolog√≠a computacional.
4.  **Gisette**
    -   Fuente: [UCI Repository](https://archive.ics.uci.edu/ml/datasets/Gisette)

> **Nota**: Se plantea la posibilidad de introducir nuevos conjuntos de datos en funci√≥n de los resultados obtenidos.

### Arquitecturas

1.  **Configuraci√≥n Cl√°sica**
    -   Algoritmo Gen√©tico (`DEAP`)
    -   Modelo de Bosques Aleatorios (`lightgbm`) para funci√≥n objetivo
    -   Sin aumentaci√≥n de datos
2.  **Configuraci√≥n Novedosa**
    -   Algoritmo Gen√©tico (`DEAP`)
    -   Autoencoders Variacionales (`pytorch`)
    -   Modelo de Bosques Aleatorios (`lightgbm`) para funci√≥n objetivo
    -   Aumentaci√≥n de datos en distintas etapas del algoritmo gen√©tico

## Diagrama de Gantt para el Cronograma de Trabajo

![Diagrama de Gantt para el Proyecto de Tesis de Maestr√≠a](data/gantt_tesis.png)

```{=html}
<!-- 

¬øQu√© presentar?
‚Ä¢ Plan de tesis
‚Ä¢ Compromiso y CV del director
‚Ä¢ CV del Tesista
Recibe y hace una primera evaluaci√≥n el Comit√© Acad√©mico. De tener el visto bueno se eleva para su evaluaci√≥n en CS. Secciones del plan de MAGstr√≠a seg√∫n normativa:
Escribir a futuro. 

Extensi√≥n m√°xima recomendada: 8 p√°ginas.

T√≠tulo
‚Ä¢ Debe expresar claramente el tema con rigurosidad y precisi√≥n t√©cnica.

Fundamentaci√≥n y justificaci√≥n del tema (extensi√≥n m√°xima 2 p√°ginas)
‚Ä¢ Marco te√≥rico.
‚Ä¢ Valor cient√≠fico del trabajo propuesto.
‚Ä¢ Alcance.

Estado del arte (extensi√≥n m√°xima sugerida 2 p√°ginas)
‚Ä¢ Evoluci√≥n hist√≥rica y actual del conocimiento.
‚Ä¢ Aspectos o conocimiento que se encuentre vacantes. 
- Citar a mis directores!

Objetivos (extensi√≥n m√°xima sugerida 1 p√°ginas) Preferentemente 1‚ÅÑ2 p√°gina
‚Ä¢ Enunciado claro de los objetivos que den fe de los alcances y l√≠mites.
‚Ä¢ OG ‚Üí l√≠nea de investigaci√≥n
‚Ä¢ OE ‚Üí resultados esperados

Metodolog√≠a y Actividades (extensi√≥n m√°xima sugerida 2 p√°ginas) Lo que necesiten
- Seleccion de datos, como se prepararon los datos, estructura de los datos y Evaluaci√≥n
- Implementaci√≥n
‚Ä¢ Actividades de desarrollar
‚Ä¢ Metodolog√≠as t√©cnicas.
Ac√° agregar cuestiones √©ticas en caso que utilicen datos de humanos (c√≥mo los anonimizan, etc.).

Cronograma de trabajo (extensi√≥n m√°xima sugerida 2 p√°ginas) ‚Üí Preferentemente 1‚ÅÑ2 o 1 p√°gina.
‚Ä¢ Listar actividades con un diagrama de Gantt

Referencias bibliogr√°ficas (extensi√≥n m√°xima sugerida 2 p√°ginas)‚Üí Unas 20-25 como m√°ximo.
‚Ä¢ Actualizadas y pertinentes
‚Ä¢ Elegir y respetar estilo (APA o IEEE)

Condiciones para el desarrollo (extensi√≥n m√°xima sugerida 1 p√°ginas) ‚Üí 1‚ÅÑ2 p√°gina
‚Ä¢ Mencionar y justificar la sede en la que se trabajar√°
‚Ä¢ Equipamiento
‚Ä¢ Software

-->
```
\pagebreak

### Bibliograf√≠a