---
format:
  pdf:
    code-fold: false    
jupyter: python3
bibliography: references.bib
---
# Institución: UTN Regional Paraná 
### Posgrado: Maestría en Minería de Datos
### Documento: Plan de Tesis
### Título: "Aumentación de datos mediante autoencoders variacionales y su impacto en estrategias de selección de características basadas en algoritmos genéticos."
### Directores: Dr. Matías Gerard y Dr. Leandro Vignolo (CONICET-UNL)
### Alumno: Lic. Claudio Sebastián Castillo (UTN)
### Fecha: Octubre 2023
---
## Fundamentación y Justificación del tema

La *selección de características* (en adelante SC) representa un desafío de optimización combinatoria complejo, que despierta interés en el universo del aprendizaje automático debido a su impacto en el rendimiento de los modelos, su generalización y la posibilidad de reducir la complejidad computacional de ciertos problemas. Tal desafío está determinado por varios factores. En primer lugar encontramos que, en espacios de alta dimensionalidad, la cardinalidad del conjunto de soluciones candidatas crece de manera exponencial, y los problemas se vuelven computacionalmente intratables debido a la extensión del espacio de búsqueda (ej. para un conjunto de `n` características es posible determinar un total de `n2` posibles soluciones). En segundo lugar, junto con la alta dimensionalidad, aparece el problema de las interacciones entre características. Aquí, el prolífico espectro de dependencias que pueden establecer los atributos plantea normalmente vínculos difíciles de modelar atento a que se multiplican de la mano de la dimensionalidad. Por último, aunque no por ello menos importante, aparece el carácter multiobjetivo de los problema de SC, donde no solo interesa maximizar la eficacia de los modelos sino también que sean eficientes. Eficiencia que implica -generalmente- la necesidad de minimizar la cantidad de atributos seleccionados para resolver un problema [@jiaoSurveyEvolutionaryMultiobjective2023].

Estos desafíos son abordados por los algoritmos genéticos (en adelante AGs) de manera conveniente, ya que se adaptan muy fácilmente a este tipo de problemas combinatorios. En efecto, los  AGs son métodos de optimización inspirados en la evolución natural, diseñados para encontrar soluciones en espacios de búsqueda complejos [@vignoloEvolutionaryLocalImprovement2017]. Dichos algoritmos son útiles en problemas donde la búsqueda exhaustiva es intratable, y resultan particularmente efectivos en espacios de búsqueda discretos, ruidosos, cuando la función objetivo no puede describirse mediante una ecuación o la misma no es diferenciable [@goldbergdavide.GeneticAlgorithmsSearch1989]. Utilizando principios basados en la evolución, estos algoritmos generan iterativamente soluciones a partir de una población de candidatos, de manera similar a cómo la evolución natural optimiza características biológicas a lo largo de generaciones en función de las condiciones del entorno. En contextos de aplicación sus resultados regularmente conducen a soluciones cercanas al óptimo, capaces de mantener un buen compromiso en la satisfacción de múltiples objetivos [@jiaoSurveyEvolutionaryMultiobjective2023]. Por eso, los AG son eficaces para atacar tanto problemas de objetivo único, como problemas multiobjetivo.

La robustez de los AG está determinada, como bien sostiene Goldberg [-@goldbergdavide.GeneticAlgorithmsSearch1989], por una serie de características distintivas, que fortalecen su configuración de búsqueda, a saber: a) operan sobre un espacio *codificado* del problema y no sobre el espacio en su representación original; b) realizan la exploración evaluando una *población de soluciones* y no soluciones individuales; c) tienen como guía una *función de aptitud* (también llamada *función de fitness*) que no requiere derivación u otras funciones de cálculo; y d) suponen *métodos probabilísticos de transición* (operadores estocásticos) y no reglas determinísticas. Estas características permiten a los AG superar restricciones que tienen otros métodos de optimización, condicionados -por ejemplo- a espacios de búsqueda continuos, diferenciables o unimodales. Por ello, su aplicación se ha difundido notablemente, trascendiendo los problemas clásicos de optimización, aplicándose en distintas tareas [@vieQualitiesChallengesFuture2021] y a lo largo de diversas aplicaciones industriales [@jiaoSurveyEvolutionaryMultiobjective2023].

En lo que respecta a su capacidad para abordar los problemas de SC es facil advertir porqué los AGs representan herramientas adecuadas. En el marco de este algoritmo cada individuo de la población representa una solución candidata, con un perfil genético particular determinado por un subconjunto de características. La búsqueda de las mejores soluciones comienza con una población inicial de individuos generados aleatoriamente, donde cada uno de ellos posee un perfil genético distintivo que corresponde a un subespacio del problema. Cada uno de estos subconjuntos se evalúa utilizando una función de aptitud, y los individuos con mejor aptitud (puntaje) tienen mayor probabilidad de ser seleccionados para la reproducción. El proceso de selección, que articula estrategias de reproducción y reemplazo generacional (más sobre esto en breve), continúa durante un cierto número de generaciones hasta que se cumple una condición de finalización [@goldbergdavide.GeneticAlgorithmsSearch1989]. Este mecanismo simple constituye un eficaz método de selección de características en contextos de alta dimensionalidad y bajo número de muestras debido a la capacidad de explorar el problema dividiéndolo en subespacios de características y, al mismo tiempo, explotar las regiones de mayor valor en cada subespacio [@goldbergdavide.GeneticAlgorithmsSearch1989]. Ambas funciones -exploración y explotación- permiten al algoritmo reconfigurar el espacio de búsqueda y poner a prueba sus complejas dependencias. 

Dicho lo anterior, no es menos cierto que la capacidad de exploración de los AGs depende de la evaluación de aptitud que orienta la búsqueda de las mejores soluciones, y tal evaluación descansa -finalmente- en la disponibilidad de datos. En efecto, la existencia y número de muestras condiciona la función de aptitud y por esa vía también al proceso de selección de características de los AGs. La disponibilidad de datos resulta así un factor clave para la selección. Este requerimiento, vinculado particularmente a la función de aptitud, se presenta no solo cuando se utiliza como evaluador a modelos complejos de aprendizaje automático (que demandan una cantidad creciente de muestras de entrenamiento [@alzubaidiSurveyDeepLearning2023]), sino también cuando se trabaja sobre datos cuyas clases se encuentran desbalanceadas [@fajardoOversamplingImbalancedData2021; @blagusSMOTEHighdimensionalClassimbalanced2013]. En ambos escenarios, que pueden darse simultáneamente, la falta de información suficiente degrada la capacidad informativa de la función de aptitud [@hastieElementStatisticalLearning2009], afectando gravemente el proceso de selección de características.

En esa línea, el problema de la disponibilidad de datos en aplicaciones de selección de características -sea dentro o fuera del campo de los AGs-, ha encontrado en las estrategias de aumentación una posible solución [@gmComprehensiveSurveyAnalysis2020]. Entre esas estrategias, los Autoencoders Variacionales (en adelante AV) han adquirido popularidad, superando a métodos tradicionales [@blagusSMOTEHighdimensionalClassimbalanced2013] y -en ciertos casos- también a otros modelos generativos también basados en redes neuronales profundas [@fajardoOversamplingImbalancedData2021].

Los AVs constituyen modelos generativos con arquitectura *encoder-decoder*, capaces de aprender una representación latente de datos observados y producir nuevas muestras con la misma distribución de probabilidad que las observaciones [@kingmaIntroductionVariationalAutoencoders2019]. Esa capacidad resulta particularmente efectiva por el hecho de que prescinde de fuertes supuestos estadísticos a los que adscriben otros modelos generativos y también por su escalabilidad (gracias al uso de *retropropagación*). Hoy los AVs son ampliamente utilizados en biología molecular, química, procesamiento de lenguaje natural y astronomía, entre otros [@ramchandranLearningConditionalVariational2022]. Estos modelos pueden presentar distintas configuraciones según el problema tratado y el objetivo particular de la implementación [@wuEVAEEvolutionaryVariational2023].

Por todo lo visto hasta aquí se advierte que la posibilidad de expandir el conjunto de datos mediante el uso de AVs abre nuevas alternativas para afrontar el problema de la selección de características aplicando AGs. Estas alternativas no solo parecen prometedoras como estrategias orientadas a la multiplicación de muestras de entrenamiento para mejorar el desempeño de la función de aptitud, sino también como partes funcionales de sus operadores de variación. De este modo, la integración de ambas herramientas ofrece un enfoque provechoso para abordar el problema de selección de características en distintas aplicaciones de aprendizaje maquinal.

A la fecha de publicación del presente trabajo no se ha encontrado experiencias de aplicación de AVs en el ámbito de selección de características mediante AGs. En la medida que esto sea así se estima que el aporte de este proyecto a la comunidad de investigadores y practicantes de la disciplina estará en proveer una metodología de integración de ambas herramientas y resultados experimentales en distintos escenarios. Dicho aporte tendría un alcance internacional a todos aquellos equipos dedicados a problemas de clasificación automática mediante herramientas de aprendizaje maquinal.

## Estado del arte

La aplicación de AVs como técnica de aumentación de datos en el contexto de distintos problemas de aprendizaje automático es extendida fuera del campo de los AGs. Se aplica al tratamiento de imágenes [@fajardoOversamplingImbalancedData2021; @aiGenerativeOversamplingImbalanced2023; @khmaissiaConfidenceGuidedDataAugmentation2023; @kwarciakDeepGenerativeNetworks2023], texto [@zhangImproveDiverseText2019] , habla [@blaauwModelingTransformingSpeech2016; @latifVariationalAutoencodersLearning2020] y música [@robertsHierarchicalLatentVector2019], y distintos formatos de datos: tabulares [@leelarathnaEnhancingRepresentationLearning2023], longitudinales [@ramchandranLearningConditionalVariational2022] y grafos [@liuConstrainedGraphVariational2018]. En lo que sigue repasaremos las experiencias más afines a nuestro enfoque sobre el impacto de la aumentación en el aprendizaje y la selección de características.

En @fajardoOversamplingImbalancedData2021 se investiga si la generación sintética de muestras mediante AVs y redes generativas antagónicas (GAN) resulta efectiva para mejorar el rendimiento de un clasificador. Para ello se crean versiones desbalanceadas de reconocidos datos multiclases MNIST [@lecunGradientBasedLearningApplied1998] y Fashion MNIST [@xiaoFashionMNISTNovelImage2017], a los cuales, posteriormente, se los re-balancea agregándoles muestras sintéticas generadas por un AV condicionado por clase (AV Condicional). Para la tarea de clasificación se emplea un Perceptrón Multicapa (MLP), y se evalúa su desempeño promediando métricas de precisión, exhaustividad y F1 sobre distintos experimentos. La evaluación incluye la comparación de resultados del clasificador con datos aumentados por sobremuestreo aleatorio, mediante SMOTE [@blagusSMOTEHighdimensionalClassimbalanced2013], GAN y AVs. El resultado muestra a los AVs -en su versión condicional- como el mejor modelo generativo para resolver el problema de datos desbalanceados mediante sobremuestreo de las clases minoritarias. 

@aiGenerativeOversamplingImbalanced2023 vuelve sobre los problemas planteados en @fajardoOversamplingImbalancedData2021, proponiendo una nueva metodología que superaría sus resultados. La solución en esta oportunidad plantea la aumentación de datos de la clase minoritaria condicionada a las características de la distribución que tienen los datos de la clase mayoritaria. El método se llama AV-Guiado-por-la-Mayoría (*Majority-Guided VAE* o MGVAE) y procura incorporar en la generación no solo información intraclase sino también interclases, con el fin de propagar la diversidad y riqueza de la mayoría en la minoría, y mitigar así riesgos de sobreajuste en los modelos. Este modelo se preentrena utilizando muestras de la clase mayoritaria, y luego se ajusta con datos de la clase minoritaria aplicando la regularización EWC [@kirkpatrickOvercomingCatastrophicForgetting2017] para retener el aprendizaje de la etapa previa. Para evaluar la eficacia de MGVAE, se realizaron experimentos en varios conjuntos de datos de imágenes y tabulares, utilizando diversas métricas de evaluación como Precisión Balanceada (B-ACC), Precisión Específica Promedio por Clase (ACSA) y Media Geométrica (GM). Los resultados muestran que MGVAE supera a otros métodos de sobremuestreo en tareas de clasificación.

Un problema diferente es tratado en @khmaissiaConfidenceGuidedDataAugmentation2023 donde se emplean AVs para aumentar datos en una tarea de clasificación con enfoque semi-supervisado. Aquí el desafío no pasa por el desbalance entre clases, sino en la búsqueda de mejorar el clasificador en regiones del espacio de características con bajo desempeño (ratios de error altos). Para eso, se mapea el espacio de características entrenando un modelo de WideResNet [@zagoruykoWideResidualNetworks2017] y luego se seleccionan las muestras mal clasificadas o con bajo nivel de confianza en la clasificación. Estas muestras se utilizan para entrenar un AV y generar datos sintéticos. Finalmente, las imágenes sintéticas se usan junto con las imágenes originales etiquetadas para entrenar un nuevo modelo de manera semi-supervisada. Se evalúan los resultados sobre STL10 y CIFAR-100 obteniendo mejoras en la clasificación de imágenes en comparación con los enfoques supervisados.

Finalmente, antecedente interesante es el presentado por @martinsVariationalAutoencodersEvolutionary2022b pues pese a no estar directamente vinculado a la aumentación de datos, incluye la generación sintética de muestras mediante AV y la selección de características por AG. En efecto, el artículo propone la generación de individuos y optimización de características orientados al diseño de proteínas (específicamente variantes de Luciferasa bacteriana *luxA*). Partiendo de muestras de ADN de proteínas se generan conjuntos de individuos combinando datos originales, datos muestreados de la capa latente -*encoder*- del AV (configurado como MSA-AV para procesar secuencias alineadas de ADN) y datos optimizados por aplicación del AG. En el caso del algoritmo genético se emplean dos tipos de optimización: de objetivo único y multiobjetivo, con funciones asociadas a la búsqueda de propiedades deseables en las muestras de ADN (solubilidad, síntesis, estabilidad y agregación de proteínas). El resultado de los experimentos realizados muestra que el diseño de proteínas guiado por la optimización mediante AG resultó en mejores soluciones que las obtenidas mediante muestreo directo, y que por su parte la optimización multiobjetivo permitió la selección de proteínas con el mejor conjunto de propiedades.

Los casos mencionados en el apartado ofrecen un conjunto de experiencias significativas a considerar al momento de resolver el problema planteado en este trabajo. Otras experiencias, como por ejemplo la configuración evolutiva de un AV [@wuEVAEEvolutionaryVariational2023], el ensamble de AVs [@leelarathnaEnhancingRepresentationLearning2023], por mencionar algunas novedosas, escapan al recorte fijado. 

## Definición del problema

La disponibilidad de datos muestrales afecta todo proceso de selección de características, y resulta particularmente condicionante en escenarios de alta dimensionalidad y bajo número de muestras. En el caso de selección de características mediante AGs la falta de datos muestrales impacta negativamente en la función de aptitud, y de esa forma limita la eficacia del algoritmo. Por eso, la técnica de aumentación de datos mediante AVs plantea una posible solución a este problema, ofreciendo distintas alternativas de implementación en el contexto de los AGs.

## Objetivos

**General**: Evaluar el impacto de la aumentación de datos mediante autoencoders variacionales sobre el desempeño de estrategias de selección de características basados en algoritmos genéticos.

**Específicos**:

1.  Implementar un algoritmo genético para selección de características.

2.  Integrar estrategias de aumentación de datos basadas en AVs en los AGs.

3.  Evaluar el desempeño de las estrategias propuestas empleando conjuntos de datos de diferente tamaño y complejidad.

4.  Comparar el desempeño de AGs con datos aumentados mediante las estrategias desarrolladas frente a implementaciones sin aumentación.

## Metodologías

Para cumplir los objetivos propuestos se comparará el desempeño de un algoritmo genético *clásico* frente a uno que incorpore las estrategias de aumentación que se desarrollen. Por un lado se implementará un algoritmo genético *clásico* aplicado a conjuntos de datos conocidos dentro de la comunidad científica y en condiciones regulares de procesamiento para la selección de características en problemas de aprendizaje automático. Por el otro, se aplicará a los mismos conjuntos un variante *novedosa* de algoritmo genético, integrando un módulo de aumentación de datos a partir de la intervención de AVs. Para evaluar esta variante de algoritmo genético se realizarán diversos experimentos con ajustes en el diseño de la arquitectura que consistirán en la integración del módulo de aumentación de datos en dos puntos: 1) previo al inicio del proceso evolutivo y 2) durante el proceso evolutivo del algoritmo genético. Para la comparación de ambas implementaciones se elegirán distintas métricas de eficacia en la clasificación, teniendo en cuenta en los resultados el tamaño final de características seleccionadas. A continuación detallamos los aspectos técnicos mencionados.

### Algoritmos

Para el presente trabajo se usarán AGs como método de búsqueda debido a la posibilidad que brindan de emplear codificación binaria y permitir así una representación intuitiva del espacio de características [@vignoloEvolutionaryLocalImprovement2017]. Otros métodos heurísticos robustos, como por ejemplo el *enjambre de partículas* (PSO) y *optimización de colonia de hormigas* (ACO), típicamente utilizan codificación basada en números reales por lo que constituyen opciones menos adecuadas al problema que enfrentaremos en este trabajo. Para aumentación de datos se utilizarán *autoencoders variacionales* (AVs) como instancia generativa [@kingmaIntroductionVariationalAutoencoders2019].

Los AGs constituyen una de las herramientas más estudiadas e implementadas dentro de los métodos evolutivos [@goldbergdavide.GeneticAlgorithmsSearch1989; @kramerGeneticAlgorithmEssentials2017], dada su capacidad para encontrar soluciones en espacios de búsqueda complejos [@vignoloEvolutionaryLocalImprovement2017]. El procedimiento de búsqueda de los AGs opera evolucionando una población de individuos que consisten en cromosomas que codifican el espacio de soluciones. Dicha evolución -al igual que la evolución natural- sucede a través de operadores (funciones) de selección, variación (mutación y cruce) y reemplazo que transforman el material genético disponible: los individuos más aptos sobreviven y se reproducen, mientras que los menos aptos desaparecen. Esta aptitud -que imita la presión selectiva de un entorno natural- se evalúa mediante la aplicación de una función objetivo (específica del problema) a cada individuo a partir de la información decodificada de sus cromosomas. Dicha función objetivo puede asumir múltiples formas [@jiaoSurveyEvolutionaryMultiobjective2023], sin perjuicio de lo cual el presente trabajo se centrará en el uso de modelos de aprendizaje automático, particularmente el Perceptrón Multicapa y Bosques Aleatorios [@breimanRandomForests2001]. Este método heurístico de búsqueda tendrá aquí dos configuraciones: una *clásica* sin aumentación de datos y una *novedosa* con aumentación de datos aplicando AVs.

Los AVs son modelos generativos implementados por redes neuronales profundas con arquitectura *encoder-decoder* capaces de aprender una representación latente de datos disponibles y generar nuevas muestras de similares características a los datos originales [@kingmaIntroductionVariationalAutoencoders2019]. Estos modelos se basan en el supuesto de que cualquier dato disponible, por ejemplo $x$, se genera mediante un proceso aleatorio que involucra una variable latente $z$. Bajo ese supuesto, el modelo procede tomando como muestra una observación de $z$ de la distribución de probabilidad *a priori* $p_\theta(z)$, que luego se utiliza para tomar una observación de $x$ de la distribución condicional $p_\theta(x|z)$. El objetivo del modelo es obtener *estimaciones de máxima verosimilitud* del parámetro $\theta$ en situaciones donde tanto la verosimilitud marginal $p_\theta(x) = \int p_\theta(z)p_\theta(x|z) dz$ como la probabilidad *a posteriori* $p_\theta(x|z)$ son intratables[^1]. Para eso, utiliza la distribución $q_\phi(z|x)$ como una aproximación al intratable $p_\theta(x|z)$, maximizando el *límite inferior variacional*[^2] para $p_\theta(x)$. El objetivo de aprendizaje del AV se da entonces por:

[^1]: Son intratables porque $z$ es una variable latente, no observada, y el cómputo de probabilidad que la incluya -en este caso $x$ - debe *marginalizar* (integrar) todo sus posibles valores, situación computacionalmente costosa en el contexto del modelos analizado.

[^2]: Limite obtenido a través de una función auxiliar conocida como función *ELBO.*

> $\mathcal{L}_{AV}(x; \theta, \phi) = \max(\phi,\theta) \left( E_{z \sim q_\phi(z|x)} [\log p_\theta (x|z)] - \text{KL}(q_\phi(z|x) \| p_\theta (z)) \right),$

donde $\text{KL}(q(\cdot) \| p(\cdot))$ denota la divergencia de Kullback--Liebler entre dos distribuciones $q(\cdot)$ y $p(\cdot)$. Una vez que el AV está entrenado, una observación sintética $x'$ se genera tomando primero $z \sim p_\theta(z)$ y posteriormente tomando $x'$ de la probabilística condicional entrenada por el modelo $p_\theta(x|z)$.

### **Conjuntos de Datos**

Se llevarán a cabo experimentos utilizando distintos conjuntos de datos con alta dimensionalidad y diferente número de características, incluyendo diversos números de clases. En principio se proponen los siguientes datos:

1.  *Madelon*: Este es un conjunto de datos artificial con 500 características, donde el objetivo es un XOR multidimensional con cinco características relevantes. Creado para el desafío NIPS 2003, está disponible en el Repositorio [UCI](http://archive.ics.uci.edu/ml/datasets.html). De las 495 características restantes, 15 corresponden a combinaciones lineales de las cinco relevantes, y las otras 480 son características de ruido. 
2.  *Leukemia*: El análisis de datos de expresión génica obtenidos de micro-datos de ADN se estudia en Golub [-@golubMolecularClassificationCancer1999] para la clasificación de tipos de cáncer. Construyeron un conjunto de datos con 7129 mediciones de expresión génica en las clases ALL (leucemia linfocítica aguda) y AML (leucemia mielogénica aguda). El problema es distinguir entre estas dos variantes de leucemia (ALL y AML). 
3.  *GCM*: El conjunto de datos GCM fue compilado en Ramaswamy [-@ramaswamyMulticlassCancerDiagnosis2001] y contiene los perfiles de expresión de 198 muestras de tumores que representan 14 clases comunes de cáncer humano. El conjunto de datos consta de un total de 190 instancias, con 16063 atributos (biomarcadores) cada una, y distribuidos en 14 clases desequilibradas. 
4.  *Gisette:* es un dataset creado para trabajar el problema de reconocimiento de dígitos escritos a mano [@isabelleguyonGisette2004]. Este conjunto de datos forma parte de los cinco conjuntos utilizados en el desafío de selección de características de NIPS 2003. Tiene 13500 observaciones y 5000 atributos. El desafío radica en diferenciar los dígitos '4' y '9', que suelen ser fácilmente confundibles entre sí. Los dígitos han sido normalizados en tamaño y centrados en una imagen fija de 28x28 píxeles. 

Se deja planteada la posibilidad de introducir nuevos conjuntos de datos en función de los resultados obtenidos y la necesidad de profundizar la evaluación de la metodología en diferentes escenarios.

### **Arquitecturas**

Para el trabajo se utilizarán dos configuraciones: una *clásica* integrada por un AG cuya función de aptitud estará conformada por un modelo de aprendizaje maquinal sin aumentación de datos y una *novedosa* con una estructura análogo de algoritmos pero con la intoducción de un módulo de aumentación de datos aplicando AVs. 

### **Métricas de evaluación**

Para la evalución y comparación de ambas implementaciones se emplearán como métricas UAR, F1 y el coeficiente de correlación de Matthews (MCC), todos ellas en relación al tamaño final de características seleccionadas por cada modelo [@jiaoSurveyEvolutionaryMultiobjective2023]. 

## Condiciones para el desarrollo

El proyecto se dividirá en dos fases: en la primera fase de desarrollo se utilizará un laptop personal (12 GB de memoria RAM y un procesador Intel Core i7 de 4 núcleos) y la segunda fase de experimentación se trabajará en Google Cloud Compute (con una configuración a medida de los experimentos, aprox. 30 GB de memoria RAM y 12 núcleos de CPU).  Respecto a software el proyecto empleará: Python 3.9. Las librerías de Python incluyen [numpy](https://github.com/numpy/numpy); [pandas](https://github.com/pandas-dev/pandas); [scikit-learn](https://github.com/scikit-learn/scikit-learn);  [LightGBM](https://github.com/microsoft/LightGBM); [DEAP](https://github.com/DEAP/deap); [PyTorch](https://github.com/pytorch/pytorch); y [MLflow](https://github.com/mlflow/mlflow). En cuanto a Conjuntos de Datos, la fuente de Madelon es el [Repositorio UCI](https://archive.ics.uci.edu/ml/datasets/Madelon), la fuente de Leucemia es el artículo @golubMolecularClassificationCancer1999 y se puede encontrar en varios repositorios de investigación en biología computacional, la fuente de GCM es el artículo @ramaswamyMulticlassCancerDiagnosis2001 y también está disponible en repositorios de biología computacional, y la fuente de Gisette es el [Repositorio UCI](https://archive.ics.uci.edu/ml/datasets/Gisette). Nota: Existe la posibilidad de introducir nuevos conjuntos de datos en función de los resultados obtenidos. 

## Cronograma de Trabajo

![Diagrama de Gantt para el Proyecto de Tesis de Maestría](data/gantt_tesis.png)

\pagebreak

### Bibliografía