---
title: "UTN Regional Paraná - MAGstría en Minería de Datos"
subtitle: "Plan de Tesis"
date: "Octubre 20223"
format:
  pdf:
    code-fold: false    
jupyter: python3
bibliography: references.bib
---

## Directores: Dr. Matías Gerard y Dr. Leandro Vignolo (CONICET-UNL)

### Título: "Aplicación de autoencoders variacionales para mejorar los procesos de optimización evolutiva multiobjetivo"

### **Alumno: Lic. Claudio Sebastián Castillo (UTN)**

\pagebreak

## Fundamentación y Justificación del tema

Los algoritmos genéticos (en adelante AG) son métodos de optimización inspirados en la evolución natural, diseñados para encontrar soluciones en espacios de búsqueda complejos [@vignoloEvolutionaryLocalImprovement2017]. A diferencia de los métodos de optimización exhaustivos (ej. métodos enumerativos [@goldbergdavide.GeneticAlgorithmsSearch1989, p.4]), los AG son particularmente efectivos en espacios de búsqueda discretos, ruidosos o cuando la función objetivo no puede describirse mediante una ecuación, o la misma no es diferenciable [@goldbergdavide.GeneticAlgorithmsSearch1989]. Utilizando principios basados en la evolución, estos algoritmos generan iterativamente soluciones a partir de una población de candidatos, de manera similar a cómo la evolución natural optimiza características biológicas a lo largo de generaciones en función de las condiciones del entorno. En contextos de aplicación sus resultados regularmente conducen a soluciones cercanas al óptimo, capaces de mantener un buen compromiso en la satisfacción de múltiples requerimientos [@jiaoSurveyEvolutionaryMultiobjective2023]. Por eso, los AG son eficaces para atacar tanto problemas de objetivo único, como problemas multiobjetivo.

La robustez de los AG está determinada, como bien sostiene Goldberg [-@goldbergdavide.GeneticAlgorithmsSearch1989], por una serie de características distintivas, que fortalecen su configuración de búsqueda, a saber: a) operan sobre un espacio *codificado* del problema y no sobre el espacio en su representación original; b) realizan la exploración evaluando una *población de soluciones* y no soluciones individuales; c) tienen como guía una *función de aptitud* que no requiere derivación u otras funciones de cálculo; y d) suponen *métodos probabilísticos de transición* (operadores estocásticos) y no reglas determinísticas. Estas características permiten a los AG superar restricciones que tienen otros métodos de optimización, condicionados -por ejemplo- a espacios de búsqueda continuos, diferenciables o unimodales. Por ello, su aplicación se ha difundido notablemente, trascendiendo los problemas clásicos de optimización, aplicándose en distintas tareas [@vieQualitiesChallengesFuture2021] y a lo largo de diversas industrias [@jiaoSurveyEvolutionaryMultiobjective2023].

La importancia de los AGs como herramientas de optimización, adquiere especial preeminencia en el problema de *selección de características* [@jiaoSurveyEvolutionaryMultiobjective2023], por lo que en este trabajo dirigiremos la atención en esa dirección*.* La *selección de características* (en adelante SC) representa un desafío de optimización combinatoria complejo, que despierta mucho interés en el universo del aprendizaje automático debido a la creciente dimensionalidad de los datos. Dicha complejidad está determinada por varios factores. En primer lugar encontramos que, en espacios de alta dimensionalidad, la cardinalidad del conjunto de soluciones candidatas crece de manera exponencial, y los problemas se vuelven computacionalmente intratables debido a la extensión del espacio de búsqueda.[^1] En segundo lugar, junto con la alta dimensionalidad, aparece el problema de las interacciones entre características. Aquí, el prolífico espectro de dependencias que pueden establecer los atributos resulta siempre difícil de modelar[^2], y crece exponencialmente de la mano de la dimensionalidad. Por último, aunque no por ello menos importante, aparece el carácter multiobjetivo de los problema de SC, donde no solo interesa maximizar la eficacia de los modelos sino también que sean eficientes. Eficiencia que implica -generalmente- la necesidad de minimizar la cantidad de atributos seleccionados para resolver un problema [@jiaoSurveyEvolutionaryMultiobjective2023].

[^1]: Cabe destacar que para un conjunto de `n` características es posible determinar un total de `n2` posibles soluciones, espacio que constituye un dominio de búsqueda difícil de cubrir aún con `n` conservadores. Por ejemplo para un conjunto de 20 características (atributos) el número total de subconjuntos a evaluar supera el millón de posibles candidatos, específicamente: 1.048.576.

[^2]: Por ejemplo, dos características con alto valor discriminatorio para resolver un problema de clasificación pueden ser redundantes debido a su correlación y exigir criterios inteligentes de inclusión-exclusión. A la inversa, características que individualmente consideradas pueden carecer de valor discriminatorio, debido a su complementariedad pueden ser esenciales para resolver un problema y por lo tanto exigir criterios complejos de evaluación y búsqueda.

Estos desafíos son abordados por los AGs de manera conveniente y creativa. Ciertamente, no son sus atributos aislados los que le dan esa posibilidad, sino la cohesión y coherencia entre ellos. Así, encontraremos que la eficacia de los AGs para tratar datos de grandes dimensiones se funda en su capacidad para explorar el problema dividiéndolo en subespacios de características y, al mismo tiempo, explotar aquellas regiones de mayor valor en cada subespacio [@goldbergdavide.GeneticAlgorithmsSearch1989]. Ambas funciones -exploración y explotación- dependen de operadores estocásticos que permiten al algoritmo reconfigurar el espacio de búsqueda y poner a prueba sus complejas dependencias. El procedimiento completo es orientado por una función de aptitud (generalmente asociada a un modelo de aprendizaje automático) que evalúa distintas soluciones y empuja (reproduce) las combinaciones de atributos que caracterizan a las mejores. Por esa razón, sin la posibilidad de contar con diversas soluciones candidatas *en competencia* no habría posibilidad alguna de búsqueda.[^3] Y en todos los casos sus resultados estarán condicionados por la disponibilidad y riqueza de tales soluciones. Consecuentemente, al condicionar el desempeño de la función objetivo, la disponibilidad de datos muestrales se vuelve un factor crítico para los AGs.

[^3]: La disponibilidad de datos es clave en contextos de optimización en el que se emplea como función objetivo modelos de redes neuronales profundas [@alzubaidiSurveyDeepLearning2023], pero también cuando -sin llegar a modelos complejos- se dispone de un conjunto de datos cuyas clases se encuentran desbalanceadas [@wongUnderstandingDataAugmentation2016, @blagusSMOTEHighdimensionalClassimbalanced2013]. En ambos escenarios, la escasez de datos afecta gravemente el proceso de optimización al limitar la capacidad informativa de la función objetivo y degradar el proceso de búsqueda [@hastieElementStatisticalLearning2009]. Aparece aquí la *maldición de la dimensionalidad* (Bellman 1961) termino que alude a los problemas que presentan los datos organizados en espacios de altas dimensiones cuando la cantidad de datos necesarios para proporcionar una representación estadísticamente confiable crece exponencialmente junto con el número de atributos.

Este desafío ha encontrado en las estrategias de **aumentación** de datos un campo de posibles respuestas [@gmComprehensiveSurveyAnalysis2020]. Y entre esas respuestas el uso de Autoencoders Variacionales (en adelante AV) ocupa un lugar central. En efecto, los AVs constituyen modelos generativos (redes neuronales profundas con arquitectura *encoder-decoder*) capaces de aprender una representación latente de datos disponibles y generar nuevas muestras con las mismas características fundamentales (i.e. similar distribución conjunta de probabilidad) que los datos originales [@kingmaIntroductionVariationalAutoencoders2019]. Esta capacidad generativa, fundada en la aptitud para modelar la distribución de probabilidad de ciertos datos, es particularmente efectiva por el hecho de que prescinde de fuertes supuestos estadísticos a los que adscriben otros modelos y por su *escalabilidad* gracias a emplear *retropropagación* como estrategia de optimización [@doerschTutorialVariationalAutoencoders2021, @kingmaIntroductionVariationalAutoencoders2019]. Hoy los AV son ampliamente utilizados en biología molecular, química, procesamiento de lenguaje natural, astronomía, entre otros [@ramchandranLearningConditionalVariational2022].

Por lo dicho hasta aquí, la posibilidad de expandir el conjunto de datos mediante el uso de AV, abre alternativas para mejorar el proceso de optimización de los AGs. Esta contribución no solo estaría en permitir una mayor exploración del espacio de soluciones aplicando operadores genéticos, sino también en mitigar riesgos de sobreajuste y convergencia prematura a soluciones subóptimas. De este modo, la integración de ambas tecnologías ofrece un enfoque prometedor para abordar escenarios de alta dimensionalidad y muestras escasas. A la fecha de publicación de la presente no hemos encontrado experiencias publicadas de aplicación general de AV en el ámbito de AG. En la medida que esto sea así creemos que nuestro aporte a la comunidad de investigadores y practicantes de la disciplina estará en proveer información y experimentación sobre la combinación de estos algoritmos largamente utilizados.

## Estado del arte

AV en general

-   [@fajardoOversamplingImbalancedData2021]

-   [@kwarciakDeepGenerativeNetworks2023]

-   [@leelarathnAGnhancingRepresentationLearning2023]

-   [@aiGenerativeOversamplingImbalanced2023]

-   [@tranArtificialNeuralNetworkbased2022]

-   [@khmaissiaConfidenceGuidedDataAugmentation2023]

AV en el contexto del AG

-   [@martinsVariationalAutoencodersEvolutionary2022]

-   [@wuEVAGEvolutionaryVariational2023]

GA y búsqueda local

-   [@vignoloEvolutionaryLocalImprovement2017]

## Definición del problema

La escasez de datos muestrales en procesos de optimización evolutiva multiobjetivo y su tratamiento a partir de técnicas de aumentación de datos mediante autoencoders variacionales.

## Objetivos

**General**:

1\. Investigar la eficacia de los autoencoders variacionales en la aumentación de datos para mejorar los procesos de optimización multiobjetivo en algoritmos genéticos.

**Específicos**:\
1. Implementar una arquitectura que integre autoencoders variacionales en la etapa inicial de un algoritmo genético para la generación aumentada de población en vistas a la optimización.\
2. Implementar una arquitectura que integre autoencoders variacionales en el contexto de optimización de un algoritmo genético, particularmente en la etapa de evolución de poblaciones, para favorecer el proceso de búsqueda de subconjuntos de características potencialmente valiosas.

## Metodologías

Para cumplir los objetivos propuestos compararemos dos algoritmos genéticos evaluando sus desempeños y resultados. Por un lado implementaremos un algoritmo genético *clásico* aplicado a conjuntos de datos conocidos dentro de la comunidad científica y en condiciones regulares de procesamiento para la resolución de problemas de clasificación. Por el otro, aplicaremos a los mismos conjuntos un variante *novedosa* de algoritmo genético, integrando un módulo de aumentación de datos a partir de la intervención de *autoencoders variacionales*. Para evaluar esta variante de algoritmo genético realizaremos diversos experimentos con ajustes en el diseño de la arquitectura que consistirán en la integración del módulo de aumentación de datos en dos puntos: 1) previo al inicio del proceso evolutivo y 2) durante el proceso evolutivo del algoritmo genético. Para la comparación de ambas implementaciones elegiremos como métricas la precisión (*accuracy*) en la clasificación y el tamaño final de características seleccionadas. A continuación detallamos los aspectos técnicos vinculados a los elementos mencionados, a saber: algoritmos, datos, arquitecturas y métricas para la evaluación.

### Algoritmos

Para el presente trabajo usaremos algoritmos genéticos (AG) como método evolutivo[^4] debido a la posibilidad que brinda este algoritmo de emplear codificación binaria y permitir así una representación intuitiva del espacio de características [@vignoloEvolutionaryLocalImprovement2017]. Para aumentación de datos utilizaremos *autoencoders variacionales* (AV) como instancia generativa [@kingmaIntroductionVariationalAutoencoders2019].

[^4]: Otros métodos genéticos robustos, como por ejemplo el *enjambre de partículas* (PSO) y *optimización de colonia de hormigas* (ACO), típicamente utilizan codificación basada en números reales por lo que constituyen opciones menos adecuadas al problema que enfrentaremos en este trabajo.

Los AGs constituyen una de las herramientas más estudiadas e implementadas dentro de los métodos evolutivos [@goldbergdavide.GeneticAlgorithmsSearch1989, @kramerGeneticAlgorithmEssentials2017], dada su capacidad para encontrar soluciones en espacios de búsqueda complejos [@vignoloEvolutionaryLocalImprovement2017]. El procedimiento de búsqueda de los AGs opera evolucionando una población de individuos que consisten en cromosomas que codifican el espacio de soluciones. Dicha evolución -al igual que la evolución natural- sucede a través de operadores (funciones) de selección, variación (mutación y cruce) y reemplazo que transforman el material genético disponible: los individuos más aptos sobreviven y se reproducen, mientras que los menos aptos desaparecen[^5]. Esta aptitud -que imita la presión selectiva de un entorno natural- se evalúa mediante la aplicación de una función objetivo (específica del problema) a cada individuo a partir de la información decodificada de sus cromosomas. Este método heurístico de búsqueda tendrá en nuestro trabajo dos configuraciones: una *clásica* sin aumentación de datos y una *novedosa* con aumentación de datos aplicando *autoencoders variacionales* (AV)*.*

[^5]: Como su nombre lo indica el operador de selección determina la elegibilidad de un individuo para sobrevivir y reproducirse en función de su aptitud para resolver un problema. En el contexto de los AGs esta aptitud no es otra cosa que el puntaje que obtiene un individuo evaluado en una función objetivo. Por su parte los operadores de variación tienen como función combinar la información genética de individuos (cruce) y alterar aleatoriamente sus cromosomas (mutación), promoviendo transformaciones en el material genético global con sesgo hacia mejorar la aptitud poblacional para resolver un problema. La variación equivale a la búsqueda natural por mejorar las adaptaciones de los individuos a su entorno. Finalmente el operador de reemplazo mantiene la población constante, sustituyendo individuos poco aptos por aquellos de mayor aptitud. Estos operadores se combinan en ciclos iterativos que se repiten hasta satisfacer un criterio de terminación deseado (por ejemplo, un número predefinido de generaciones o un valor de aptitud) [@vignoloEvolutionaryLocalImprovement2017].

Para aumentar los AG emplearemos *autoencoders variacionales* (AVs). Los AVs son modelos generativos implementados por redes neuronales profundas con arquitectura *encoder-decoder* capaces de aprender una representación latente de datos disponibles y generar nuevas muestras de similares características a los datos originales [@kingmaIntroductionVariationalAutoencoders2019]. Estos modelos se basan en el supuesto de que cualquier dato disponible, por ejemplo $x$, se genera mediante un proceso aleatorio que involucra una variable latente $z$. Bajo ese supuesto, el modelo procede tomando como muestra una observación de $z$ de la distribución de probabilidad *a priori* $p_\theta(z)$, que luego se utiliza para tomar una observación de $x$ de la distribución condicional $p_\theta(x|z)$. El objetivo del modelo es obtener *estimaciones de máxima verosimilitud* del parámetro $\theta$ en situaciones donde tanto la verosimilitud marginal $p_\theta(x) = \int p_\theta(z)p_\theta(x|z) dz$ como la probabilidad *a posteriori* $p_\theta(x|z)$ son intratables[^6]. Para eso, utiliza la distribución $q_\phi(z|x)$ como una aproximación al intratable $p_\theta(x|z)$, maximizando el *límite inferior variacional*[^7] para $p_\theta(x)$. El objetivo de aprendizaje del AV se da entonces por:

[^6]: Son intratables porque $z$ es una variable latente, no observada, y el cómputo de probabilidad que la incluya -en este caso $x$ - debe *marginalizar* (integrar) todo sus posibles valores, situación computacionalmente costosa en el contexto del modelos analizado.

[^7]: Limite obtenido a través de una función auxiliar conocida como función *ELBO.*

> $\mathcal{L}_{AV}(x; \theta, \phi) = \max(\phi,\theta) \left( E_{z \sim q_\phi(z|x)} [\log p_\theta (x|z)] - \text{KL}(q_\phi(z|x) \| p_\theta (z)) \right)$

donde $\text{KL}(q(\cdot) \| p(\cdot))$ denota la divergencia de Kullback--Liebler entre dos distribuciones $q(\cdot)$ y $p(\cdot)$. Una vez que el AV está entrenado, una observación sintética $x'$ se genera tomando primero $z \sim p_\theta(z)$ y posteriormente tomando $x'$ de la probabilística condicional entrenada por el modelo $p_\theta(x|z)$.

### **Conjuntos de Datos**

Se llevarán a cabo experimentos utilizando cuatro conjuntos de datos con alta dimensionalidad y diferentes número de características, incluyendo un diverso número clases. Esos datos son:

1.  *Madelon*: Este es un conjunto de datos artificial con 500 características, donde el objetivo es un XOR multidimensional con cinco características relevantes. Fue creado para el desafío de Selección de Características NIPS 2003[^8], y está disponible en el Repositorio UCI[^9]. De las 495 características restantes, 15 corresponden a combinaciones lineales de las cinco relevantes, y las otras 480 son características de ruido. Madelon es un problema de clasificación de dos clases con variables de entrada binarias dispersas. Las dos clases están equilibradas, y los datos se dividen en conjuntos de entrenamiento y prueba.
2.  *Leukemia*: El análisis de datos de expresión génica obtenidos de micro-datos de ADN se estudia en Golub [-@golubMolecularClassificationCancer1999] para la clasificación de tipos de cáncer. Construyeron un conjunto de datos con 7129 mediciones de expresión génica en las clases ALL (leucemia linfocítica aguda) y AML (leucemia mielogénica aguda). El problema es distinguir entre estas dos variantes de leucemia (ALL y AML). Los datos se dividen originalmente en dos subconjuntos: un conjunto de entrenamiento y un conjunto de prueba independiente. El conjunto de entrenamiento consta de 38 muestras (27 ALL y 11 AML) de especímenes de médula ósea. El conjunto de prueba tiene 34 muestras (20 ALL y 14 AML), preparadas bajo diferentes condiciones experimentales e incluyendo 24 especímenes de médula ósea y 10 muestras de sangre.
3.  *GCM*: El conjunto de datos GCM fue compilado en Ramaswamy [-@ramaswamyMulticlassCancerDiagnosis2001] y contiene los perfiles de expresión de 198 muestras de tumores que representan 14 clases comunes de cáncer humano3. Aquí el enfoque estuvo en 190 muestras de tumores después de excluir 8 muestras de metástasis, y el preprocesamiento se realizó de acuerdo con \[24\]. Finalmente, cada matriz se estandarizó a una media de 0 y una varianza de 1 según \[25\]. El conjunto de datos consta de un total de 190 instancias, con 16063 atributos (biomarcadores) cada una, y distribuidos en 14 clases desequilibradas. En nuestros experimentos, los datos se separaron en tres conjuntos: entrenamiento (72 muestras), validación (72 muestras) y prueba (42 muestras). Con el objetivo de equilibrar las clases en el conjunto de entrenamiento y prevenir el sobreajuste del clasificador debido a la clase mayoritaria, las muestras se repitieron para obtener el mismo número para cada clase. Al final, se utilizaron un total de 168 muestras para el entrenamiento. Por el contrario, los conjuntos de validación y prueba están estratificados, manteniendo el desequilibrio de clases.
4.  *Gisette:* es un dataset creado para trabajar el problema de reconocimiento de dígitos escritos a mano [@isabelleguyonGisette2004]. Este conjunto de datos forma parte de los cinco conjuntos utilizados en el desafío de selección de características de NIPS 2003. Tiene 13500 observaciones y 5000 atributos. El desafío radica en diferenciar los dígitos '4' y '9', que suelen ser fácilmente confundibles entre sí. Los dígitos han sido normalizados en tamaño y centrados en una imagen fija de 28x28 píxeles. Para el desafío de selección de características, se modificaron los datos originales. Específicamente, se muestrearon píxeles al azar en la parte superior central de la característica que contiene la información necesaria para diferenciar el 4 del 9. Además, se crearon características de orden superior como productos de estos píxeles para sumergir el problema en un espacio de características de mayor dimensión. También se añadieron características distractoras denominadas "sondas", que no tienen poder predictivo. El orden de las características y patrones fue aleatorizado.

[^8]: <http://clopinet.com/isabelle/Projects/NIPS2003/>

[^9]: <http://archive.ics.uci.edu/ml/datasets.html>

### **Arquitecturas**

Para nuestro trabajo utilizaremos dos arquitecturas: una *clásica* sin aumentación de datos y una *novedosa* con aumentación de datos aplicando *autoencoders variacionales* (AV). A su vez, dentro de esta segunda configuración ensayaremos ajustes en el diseño que consistirán en la integración del módulo de aumentación de datos en dos puntos: 1) previo al inicio del proceso evolutivo y 2) durante el proceso evolutivo del algoritmo genético. De esta forma buscamos evaluar el aporte generativo del modelo AV en distintas etapas del AG.

### **Métricas de evaluación**

Para la comparación de ambas implementaciones elegiremos como métricas la precisión en la clasificación (*accuracy*) y el tamaño final de características seleccionadas por el modelo genético. Generalmente los AGs constituye modelos *multiobjetivo* donde no solo interesa optimizar una métrica particular vinculada a su eficacia -por ejemplo precisión en su clasificación de individuos- sino también interesa satisfacer requerimientos de eficiencia -como por ejemplo minimizar el conjunto de características con que opera el modelo- [@jiaoSurveyEvolutionaryMultiobjective2023]. Por esta razón nuestra evaluación tomará en cuenta ambas dimensiones en el desempeño de las soluciones propuestas.

## Cronograma de trabajo

\[pendiente\]

## Condiciones para el desarrollo

\[pendiente\]

```{=html}
<!-- 

¿Qué presentar?
• Plan de tesis
• Compromiso y CV del director
• CV del Tesista
Recibe y hace una primera evaluación el Comité Académico. De tener el visto bueno se eleva para su evaluación en CS. Secciones del plan de MAGstría según normativa:
Escribir a futuro. 

Extensión máxima recomendada: 8 páginas.

Título
• Debe expresar claramente el tema con rigurosidad y precisión técnica.

Fundamentación y justificación del tema (extensión máxima 2 páginas)
• Marco teórico.
• Valor científico del trabajo propuesto.
• Alcance.

Estado del arte (extensión máxima sugerida 2 páginas)
• Evolución histórica y actual del conocimiento.
• Aspectos o conocimiento que se encuentre vacantes. 
- Citar a mis directores!

Objetivos (extensión máxima sugerida 1 páginas) Preferentemente 1⁄2 página
• Enunciado claro de los objetivos que den fe de los alcances y límites.
• OG → línea de investigación
• OE → resultados esperados

Metodología y Actividades (extensión máxima sugerida 2 páginas) Lo que necesiten
- Seleccion de datos, como se prepararon los datos, estructura de los datos y Evaluación
- Implementación
• Actividades de desarrollar
• Metodologías técnicas.
Acá agregar cuestiones éticas en caso que utilicen datos de humanos (cómo los anonimizan, etc.).

Cronograma de trabajo (extensión máxima sugerida 2 páginas) → Preferentemente 1⁄2 o 1 página.
• Listar actividades con un diagrama de Gantt

Referencias bibliográficas (extensión máxima sugerida 2 páginas)→ Unas 20-25 como máximo.
• Actualizadas y pertinentes
• Elegir y respetar estilo (APA o IEEE)

Condiciones para el desarrollo (extensión máxima sugerida 1 páginas) → 1⁄2 página
• Mencionar y justificar la sede en la que se trabajará
• Equipamiento
• Software

-->
```
\pagebreak

### Bibliografía